{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_args_key = \"live\"\n",
    "proof_number = 0\n",
    "keynumber0816 = 4\n",
    "yesorno_inputdim = 50  \n",
    "kkkkkkkkkkkk = 0   \n",
    "key_read_write = 'write'  \n",
    "raise_Exception = False \n",
    "batch_sanpling_bur = True  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.0+cu113\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext.legacy import data, datasets\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "import copy\n",
    "# from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# from model_nn import RNN, RNN_gradient,RNN50, RNN50_gradient\n",
    "# from args_fedavg import args_parser_live, args_parser_shake\n",
    "from model_nn_min import RNN, RNN_gradient,RNN50, RNN50_gradient\n",
    "from args_fvg import args_parser_live, args_parser_shake\n",
    "\n",
    "from nltk.tag import pos_tag\n",
    "import re\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/workspace/fedavg_ceshi/out_log/log_live_0_4_0_30_0.5_write_08_30_2023_06_19_08_log.txt\n",
      "/mnt/workspace/fedavg_ceshi/out_pth/live_0_4_0_30_0.5_write_08_30_2023_06_19_08_dict.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_log = r\"/mnt/workspace/fedavg_ceshi/out_log\"\n",
    "out_pth = r\"/mnt/workspace/fedavg_ceshi/out_pth\"\n",
    "write_read_txt = r\"/mnt/workspace/fedavg_ceshi/write_read_txt\"\n",
    "\n",
    "if file_args_key == \"shake\":\n",
    "    args = args_parser_shake()\n",
    "elif file_args_key == \"live\":\n",
    "    args = args_parser_live()\n",
    "\n",
    "String = time.strftime(\"%m_%d_%Y_%H_%M_%S\",time.localtime())\n",
    "namestr = str(String)\n",
    "\n",
    "file_write_log_name = out_log + \"/\" + \"log_\" + file_args_key + '_'  + str(proof_number) \\\n",
    "    + '_'+ str(keynumber0816) + '_'+ str(kkkkkkkkkkkk) + '_' \\\n",
    "        + str(args.B) + '_'+ str(args.C) + '_' + key_read_write + '_'+ namestr + \"_log.txt\"\n",
    "print(file_write_log_name)\n",
    "\n",
    "file_write_log = open(file_write_log_name, \"w\", encoding='utf-8')\n",
    "save_name_pth = out_pth + \"/\" +  file_args_key + '_'  + str(proof_number) \\\n",
    "    + '_'+ str(keynumber0816) + '_'+ str(kkkkkkkkkkkk) + '_' \\\n",
    "        + str(args.B) + '_'+ str(args.C) + '_' + key_read_write + '_'+ namestr +'_dict.pth'\n",
    "print(save_name_pth)\n",
    "\n",
    "writecon = str(save_name_pth) + \"\\n\"\n",
    "file_write_log.write(writecon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of train data: 25000\n",
      "len of test data: 25000\n",
      "['I', 'was', 'five', 'when', 'the', 'show', 'made', 'its', 'debut', 'in', '1958', 'and', 'at', 'a', 'later', 'point', ',', 'was', 'a', 'regular', 'viewer', '.', 'I', 'remember', 'that', 'I', 'really', 'enjoyed', 'the', 'show', ',', 'along', 'with', '\"', 'Leave', 'It', 'To', 'Beaver', '\"', ',', '\"', 'My', 'Three', 'Sons', '\"', ',', '\"', 'Ozzie', 'and', 'Harriet', '\"', ',', '\"', 'Dick', 'Van', 'Dyke', '\"', ',', 'reruns', 'of', '\"', 'I', 'Love', 'Lucy', '\"', ',', '\"', 'The', 'Real', 'McCoys', '\"', ',', 'etc', '.', 'I', 'am', 'now', 'enjoying', 'the', 'first', 'season', 'of', '\"', 'Donna', 'Reed', '\"', 'on', 'DVD', 'and', 'have', 'watched', 'the', 'first', 'two', 'episodes', '.', 'Donna', 'Stone', 'is', 'shown', 'to', 'be', 'an', 'intelligent', ',', 'well', '-', 'mannered', ',', 'problem', '-', 'solving', ',', 'serene', ',', 'stay', '-', 'at', '-', 'home', 'mom', ',', 'similar', 'to', 'June', 'Cleaver', 'and', 'in', 'contrast', 'to', 'Lucy', 'Ricardo', '.', 'In', 'episode', '2', ',', 'I', 'especially', 'like', 'how', 'Ms.', 'Reed', 'becomes', 'a', 'surrogate', 'dad', ',', 'trading', 'in', 'her', 'dress', 'for', 'sweats', 'and', 'boxing', 'gloves', ',', 'while', 'teaching', 'her', 'son', 'how', 'to', 'defend', 'himself', 'physically', 'against', 'a', 'much', 'larger', 'bully', '.', 'While', 'none', 'of', 'the', 'mothers', 'in', 'the', 'neighborhood', 'I', 'grew', 'up', 'in', ',', 'including', 'my', 'own', ',', 'exactly', 'met', 'the', 'idealistic', 'standards', 'portrayed', 'by', 'Ms.', 'Reed', ',', 'it', 'is', 'refreshing', 'to', 'see', 'good', 'manners', 'and', 'intelligent', 'decision', '-', 'making', 'prevail', 'at', 'the', 'end', 'of', 'the', 'day', ',', 'in', 'contrast', 'to', 'today', \"'s\", 'accepted', 'standards', 'of', 'vulgarity', ',', 'selfishness', 'and', 'indifference', 'among', 'one', \"'s\", 'neighbors', '.', 'I', 'can', 'not', 'imagine', 'Jeff', 'and', 'Mary', 'Stone', 'being', 'told', 'by', 'their', 'parents', 'that', 'trespassing', 'in', 'their', 'neighbors', \"'\", 'yards', 'is', 'okay', ',', 'leaving', 'a', 'dog', 'outside', 'to', 'bark', 'all', 'day', 'is', 'acceptable', ',', 'or', 'telling', 'their', 'mother', 'to', '\"', 'shut', 'up', '\"', 'in', 'a', 'supermarket', 'in', 'front', 'of', 'everyone', '.']\n",
      "pos\n"
     ]
    }
   ],
   "source": [
    "TEXT = data.Field(tokenize='spacy')\n",
    "LABEL = data.LabelField(dtype=torch.float)\n",
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
    "\n",
    "if file_args_key == \"shake\":\n",
    "    # train_data.examples = train_data.examples[:len(train_data.examples)//500]\n",
    "    # test_data.examples = test_data.examples[:len(test_data.examples)//500]\n",
    "\n",
    "    train_data.examples = train_data.examples[:len(train_data.examples)//125]\n",
    "    test_data.examples = test_data.examples[:len(test_data.examples)//125]\n",
    "\n",
    "len_train_data = len(train_data)\n",
    "len_test_data = len(test_data)\n",
    "\n",
    "print('len of train data:', len(train_data))\n",
    "print('len of test data:', len(test_data))\n",
    "print(test_data.examples[1].text) \n",
    "print(test_data.examples[1].label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import nltk\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "a = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10002 torch.Size([10002, 50])\n"
     ]
    }
   ],
   "source": [
    "if yesorno_inputdim == 100:\n",
    "    TEXT.build_vocab(train_data, max_size=10000, vectors='glove.6B.100d')\n",
    "if yesorno_inputdim == 50:\n",
    "    TEXT.build_vocab(train_data, max_size=10000, vectors='glove.6B.50d')\n",
    "LABEL.build_vocab(train_data)\n",
    "vocab_size =  len(TEXT.vocab)\n",
    "pretrained_embedding = TEXT.vocab.vectors\n",
    "\n",
    "print(vocab_size, pretrained_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RFFM(i, x):\n",
    "    torch.manual_seed(i)\n",
    "    input_dim = 100\n",
    "    output_dim = 50\n",
    "    sigma = 1.0\n",
    "    omega = torch.randn(output_dim, input_dim, device=args.device) * sigma\n",
    "    b = torch.rand(output_dim, device=args.device) * 2 * np.pi\n",
    "    x = x.view(-1, input_dim).to(args.device)\n",
    "    z = torch.cos(torch.mm(x, omega.t()) + b)\n",
    "    Z_Out =  torch.sqrt(2.0 / torch.tensor(output_dim)) * z\n",
    "    return Z_Out.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "if yesorno_inputdim == 100:\n",
    "    new_vectors = []\n",
    "    for i, word in enumerate(TEXT.vocab.itos):\n",
    "        old_vector = pretrained_embedding[i]\n",
    "        new_vector = RFFM(i, old_vector)\n",
    "        new_vectors.append(new_vector)\n",
    "\n",
    "    new_pretrained_embedding = torch.stack(new_vectors, dim=0)\n",
    "    new_pretrained_embedding = new_pretrained_embedding.reshape(-1,50)\n",
    "    print(new_pretrained_embedding.shape)\n",
    "elif yesorno_inputdim == 50:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_vector_50(word):\n",
    "    word_index = TEXT.vocab.stoi[word]\n",
    "    word_vector = new_pretrained_embedding[word_index]\n",
    "    return word_vector\n",
    "\n",
    "\n",
    "def proof1(a, b, epcluo):\n",
    "    a = a.to(torch.float)\n",
    "    b = b.to(torch.float)\n",
    "\n",
    "    dot_product = torch.dot(a, b)\n",
    "    norm_a = torch.norm(a, p=2)\n",
    "    norm_b = torch.norm(b, p=2)\n",
    "    similarity = dot_product / (norm_a * norm_b)\n",
    "    return torch.exp(similarity* epcluo / 2.0 *(-1))\n",
    "\n",
    "def proof1_change(word_list,  proper_nouns, filtered, proper_nouns_pos):\n",
    "    if len(proper_nouns) == 0 or len(filtered) == 0 : \n",
    "        return word_list\n",
    "    epcluo = args.epcluo\n",
    "    gam = []\n",
    "\n",
    "    xlist = [word_vector_50(word) for word in proper_nouns]\n",
    "    ylist = [word_vector_50(word) for word in filtered]\n",
    "    \n",
    "    for x in xlist:\n",
    "        x_list = []\n",
    "        for y in ylist:\n",
    "            res = proof1(x, y, epcluo)\n",
    "            x_list.append(res.item())\n",
    "        gam.append(x_list)\n",
    "\n",
    "    gam_tensor = torch.tensor(gam)\n",
    "    row_sums = torch.sum(gam_tensor, dim=1, keepdim=True)  \n",
    "    result_prob = torch.div(gam_tensor, row_sums)  \n",
    "    sample = torch.multinomial(result_prob, 1)\n",
    "    list_result = sample.squeeze().tolist()\n",
    "    try:\n",
    "        list_result = list(list_result)\n",
    "    except:\n",
    "        list_result = [list_result]\n",
    "\n",
    "    rresul = [filtered[i] for i in list_result]\n",
    "    iiindex = 0\n",
    "    for index, word in enumerate(proper_nouns_pos):\n",
    "        if (word[0] in proper_nouns) and (word[1].startswith('NNP') or word[1].startswith('NNPS') ):\n",
    "            newword = rresul[iiindex] \n",
    "            word_list[index] = newword\n",
    "            iiindex += 1   \n",
    "    \n",
    "    return word_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_vector_100(word):\n",
    "    word_index = TEXT.vocab.stoi[word]\n",
    "    word_vector = TEXT.vocab.vectors[word_index]\n",
    "    return word_vector\n",
    "\n",
    "def proof2(x, y, epcluo):\n",
    "    K = torch.exp(torch.norm(x-y, p=2).pow(2) / 2.0 *(-1) )\n",
    "    result = K * epcluo / 2.0 *(-1) \n",
    "    e14 = torch.exp(result)\n",
    "    return e14\n",
    "\n",
    "def proof2_change(word_list,  proper_nouns, filtered, proper_nouns_pos):\n",
    "    if len(proper_nouns) == 0 or len(filtered) == 0 :  \n",
    "        return word_list\n",
    "    epcluo = args.epcluo\n",
    "    gam = []\n",
    "\n",
    "    xlist = [word_vector_100(word) for word in proper_nouns]\n",
    "    ylist = [word_vector_100(word) for word in filtered]\n",
    "    \n",
    "\n",
    "    for x in xlist:\n",
    "        x_list = []\n",
    "        for y in ylist:\n",
    "            res = proof2(x, y, epcluo)\n",
    "            x_list.append(res.item())\n",
    "        gam.append(x_list)\n",
    "\n",
    "    gam_tensor = torch.tensor(gam)\n",
    "    row_sums = torch.sum(gam_tensor, dim=1, keepdim=True)  \n",
    "    result_prob = torch.div(gam_tensor, row_sums)  \n",
    "    sample = torch.multinomial(result_prob, 1)\n",
    "    list_result = sample.squeeze().tolist()\n",
    "    try:\n",
    "        list_result = list(list_result)\n",
    "    except:\n",
    "        list_result = [list_result]\n",
    "    rresul = [filtered[i] for i in list_result]\n",
    "    iiindex = 0\n",
    "    \n",
    "    for index, word in enumerate(proper_nouns_pos):\n",
    "        if (word[0] in proper_nouns) and (word[1].startswith('NNP') or word[1].startswith('NNPS') ):\n",
    "            newword = rresul[iiindex]\n",
    "            word_list[index] = newword\n",
    "            iiindex += 1\n",
    "\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proof3(zx, zy, epcluo): \n",
    "    result = (torch.dot(zx, zy)) * epcluo / 4.0 *(-1) \n",
    "    e14 = torch.exp(result)\n",
    "    return e14\n",
    "\n",
    "def proof3_change(word_list,  proper_nouns, filtered, proper_nouns_pos):\n",
    "    if len(proper_nouns) == 0 or len(filtered) == 0 :  \n",
    "        return word_list\n",
    "    epcluo = args.epcluo\n",
    "    gam = []\n",
    "    xlist = [word_vector_50(word) for word in proper_nouns]\n",
    "    ylist = [word_vector_50(word) for word in filtered]\n",
    "    \n",
    "    for x in xlist:\n",
    "        x_list = []\n",
    "        for y in ylist:\n",
    "            res = proof3(x, y, epcluo)\n",
    "            x_list.append(res.item())\n",
    "        gam.append(x_list)\n",
    "\n",
    "    gam_tensor = torch.tensor(gam)\n",
    "    row_sums = torch.sum(gam_tensor, dim=1, keepdim=True)  \n",
    "    result_prob = torch.div(gam_tensor, row_sums)  \n",
    "    sample = torch.multinomial(result_prob, 1)\n",
    "    list_result = sample.squeeze().tolist()\n",
    "    try:\n",
    "        list_result = list(list_result)\n",
    "    except:\n",
    "        list_result = [list_result]\n",
    "\n",
    "    rresul = [filtered[i] for i in list_result]\n",
    "    iiindex = 0\n",
    "\n",
    "    for index, word in enumerate(proper_nouns_pos):\n",
    "        if (word[0] in proper_nouns) and (word[1].startswith('NNP') or word[1].startswith('NNPS') ):\n",
    "            newword = rresul[iiindex]\n",
    "            word_list[index] = newword\n",
    "            iiindex += 1\n",
    "\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proof6(zx, zy, epcluo):\n",
    "    result = (torch.dot(zx, zy)) * epcluo / 4.0 *(-1) \n",
    "    e14 = torch.exp(result)\n",
    "    return e14\n",
    "\n",
    "def proof6_change(word_list,  proper_nouns, filtered, proper_nouns_pos, filtered_y, proper_nouns_x ):\n",
    "    if len(proper_nouns) == 0 or len(filtered) == 0 :  \n",
    "        return word_list\n",
    "\n",
    "    epcluo = args.epcluo\n",
    "    gam = []\n",
    "    xlist = [word_vector_50(word) for word in proper_nouns]\n",
    "    ylist = [word_vector_50(word) for word in filtered]\n",
    "    ylist_f = [word_vector_50(word) for word in filtered_y]\n",
    "    xlist_f = [word_vector_50(word) for word in proper_nouns_x]\n",
    "\n",
    "    for x in xlist:\n",
    "        x_list = []\n",
    "        for y in ylist:\n",
    "            res = proof6(x, y, epcluo)\n",
    "            x_list.append(res.item())\n",
    "        gam.append(x_list)\n",
    "\n",
    "    gam_tensor = torch.tensor(gam)\n",
    "    row_sums = torch.sum(gam_tensor, dim=1, keepdim=True)  \n",
    "    result_prob = torch.div(gam_tensor, row_sums) \n",
    "    sample = torch.multinomial(result_prob, 1)\n",
    "    tensor = sample.clone()\n",
    "    list_result = tensor.squeeze().tolist()\n",
    "\n",
    "    try:\n",
    "        list_result = list(list_result)\n",
    "    except:\n",
    "        list_result = [list_result]\n",
    "\n",
    "    rresul = [filtered[i] for i in list_result]\n",
    "    pr = args.pr\n",
    "    gam_pr = []\n",
    "\n",
    "    for y in ylist_f:\n",
    "        x_list_new = []\n",
    "        for x in xlist_f:\n",
    "            res = proof6(y, x, epcluo)\n",
    "            x_list_new.append(res.item())\n",
    "        gam_pr.append(x_list_new)\n",
    "\n",
    "    gam_pr_tensor = torch.tensor(gam_pr)\n",
    "    row_pr_sums = torch.sum(gam_pr_tensor, dim=1, keepdim=True)  \n",
    "    result_prob = pr * torch.div(gam_pr_tensor, row_pr_sums)  \n",
    "    column_to_add = torch.full((result_prob.shape[0], 1), pr)\n",
    "    tensor_with_column = torch.cat((result_prob, column_to_add), dim=1)\n",
    "    sample_pr = torch.multinomial(tensor_with_column, 1)\n",
    "    tensor_pr = sample_pr.clone()\n",
    "    list_result_pr = tensor_pr.squeeze().tolist()\n",
    "\n",
    "    try:\n",
    "        list_result_pr = list(list_result_pr)\n",
    "    except:\n",
    "        list_result_pr = [list_result_pr]\n",
    "\n",
    "    rresuly = []\n",
    "    xylong = len(tensor_with_column[0,:]) - 1\n",
    "\n",
    "    for index, yword in enumerate(list_result_pr):\n",
    "        if yword == xylong :\n",
    "            rresuly.append(filtered_y[index])\n",
    "        else:\n",
    "            rresuly.append(proper_nouns_x[yword])\n",
    "    iiindex = 0\n",
    "    iiindey = 0\n",
    "    \n",
    "    for index, word in enumerate(proper_nouns_pos):\n",
    "        if (word[0] in proper_nouns) and (word[1].startswith('NNP') or word[1].startswith('NNPS') ):\n",
    "            newword = rresul[iiindex]\n",
    "            word_list[index] = newword\n",
    "            iiindex += 1\n",
    "\n",
    "        elif (word[0] in filtered_y) and not (word[1].startswith('NNP') or word[1].startswith('NNPS')) :  \n",
    "            newword = rresuly[iiindey]\n",
    "            word_list[index] = newword\n",
    "            iiindey += 1\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_proper_nouns(words):\n",
    "    tagged_words = pos_tag(words)\n",
    "    proper_nouns = []\n",
    "    filtered_y = []\n",
    "    proper_nouns_pos = []\n",
    "\n",
    "    for word, pos in tagged_words:\n",
    "        if pos.startswith('NNP') or pos.startswith('NNPS'):\n",
    "            proper_nouns.append(word)\n",
    "        else:\n",
    "            filtered_y.append(word)\n",
    "        proper_nouns_pos.append((word,pos))\n",
    "    return proper_nouns, proper_nouns_pos, filtered_y\n",
    "\n",
    "\n",
    "def zxy_split(word_list, proof_number):\n",
    "    proper_nouns, proper_nouns_pos, filtered_y = filter_proper_nouns(word_list)\n",
    "    filtered = list(set(word_list) - set(proper_nouns))\n",
    "\n",
    "    if  proof_number == 1:\n",
    "        new_word_list = proof1_change(word_list,  proper_nouns, filtered, proper_nouns_pos)\n",
    "\n",
    "    elif proof_number == 2:\n",
    "        new_word_list = proof2_change(word_list,  proper_nouns, filtered, proper_nouns_pos)\n",
    "\n",
    "    elif proof_number == 3:\n",
    "        new_word_list = proof3_change(word_list,  proper_nouns, filtered, proper_nouns_pos)\n",
    "\n",
    "    elif proof_number == 6:\n",
    "        proper_nouns_x = list(set(proper_nouns))\n",
    "        new_word_list = proof6_change(word_list,  proper_nouns, filtered, proper_nouns_pos, filtered_y, proper_nouns_x)\n",
    "    elif proof_number == 0:    \n",
    "        new_word_list = word_list\n",
    "    else:\n",
    "        pass\n",
    "    return new_word_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geshj_tra_tes(data, train_or_testa):\n",
    "    for index in range(len(data)):\n",
    "        if ((index+1) % 1000 == 0):\n",
    "            print(index)\n",
    "\n",
    "        atexta = data.examples[index].text\n",
    "        filtered_aaa0 = [word for word in atexta if re.match(r'^[A-Za-z.,!?0-9]+$', word)]\n",
    "        if train_or_testa == \"oftrain\":\n",
    "            word_listaaa = zxy_split(filtered_aaa0, proof_number)\n",
    "            data.examples[index].text = word_listaaa\n",
    "        elif train_or_testa == \"oftesta\":\n",
    "            data.examples[index].text = filtered_aaa0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chulia(astring):\n",
    "    astring = astring.strip('\\n').split('#')\n",
    "    astring = [i for i in astring if len(i)>0]\n",
    "    return astring\n",
    "\n",
    "def train_test_read(tr_te, data):\n",
    "    for index in range(len(tr_te)): \n",
    "        \n",
    "        word_listaaa = data[index]\n",
    "        tr_te.examples[index].text = word_listaaa\n",
    "        if (file_args_key == \"live\") and (index % 1000 == 0):\n",
    "            print(index)\n",
    "        if (file_args_key == \"shake\") and (index % args.B == 0):\n",
    "            print(index)\n",
    "\n",
    "def read_or_write(key, proof_number):\n",
    "    # file_args_key = \"live\"\n",
    "    # file_args_key = \"shake\"\n",
    "    if proof_number == 1:\n",
    "        if file_args_key == \"live\":\n",
    "            file_name_train = 'train_data_change_result_proof1_norm_dim50.txt'\n",
    "            file_name_test  = 'test_data_change_result_proof1_norm_dim50.txt'\n",
    "        elif file_args_key == \"shake\":\n",
    "            file_name_train = 'train_proof1_50_norm_dim50.txt'\n",
    "            file_name_test = 'test__proof1_50_norm_dim50.txt'\n",
    "\n",
    "    if proof_number == 2:\n",
    "        if file_args_key == \"live\":\n",
    "            file_name_train = 'train_data_change_result_proof2_norm_dim50.txt'\n",
    "            file_name_test = 'test_data_change_result_proof2_norm_dim50.txt'\n",
    "        elif file_args_key == \"shake\":\n",
    "            file_name_train = 'train_proof2_50_norm_dim50.txt'\n",
    "            file_name_test = 'test__proof2_50_norm_dim50.txt'\n",
    "\n",
    "    if proof_number == 3:\n",
    "        if file_args_key == \"live\":\n",
    "            file_name_train = 'train_data_change_result_proof3_norm_dim50.txt'\n",
    "            file_name_test = 'test_data_change_result_proof3_norm_dim50.txt'\n",
    "        elif file_args_key == \"shake\":\n",
    "            file_name_train = 'train_proof3_50_norm_dim50.txt'\n",
    "            file_name_test = 'test__proof3_50_norm_dim50.txt'\n",
    "\n",
    "    if proof_number == 6:\n",
    "        if file_args_key == \"live\":\n",
    "            file_name_train = 'train_data_change_result_proof6_norm_dim50.txt'\n",
    "            file_name_test  = 'test_data_change_result_proof6_norm_dim50.txt'\n",
    "        elif file_args_key == \"shake\":\n",
    "            file_name_train = 'train_proof6_50_norm_dim50.txt'\n",
    "            file_name_test = 'test__proof6_50_norm_dim50.txt'\n",
    "    \n",
    "    if proof_number == 0:\n",
    "        if file_args_key == \"live\":\n",
    "            file_name_train = 'train_data_change_result_proof0_norm_dim50.txt'\n",
    "            file_name_test  = 'test_data_change_result_proof0_norm_dim50.txt'\n",
    "        elif file_args_key == \"shake\":\n",
    "            file_name_train = 'train_proof0_50_norm_dim50.txt'\n",
    "            file_name_test = 'test__proof0_50_norm_dim50.txt'\n",
    "\n",
    "    file_name_train = write_read_txt + \"/\" + file_name_train\n",
    "    file_name_test = write_read_txt + \"/\" + file_name_test\n",
    "\n",
    "    if key=='read':\n",
    "        fead_train = open(file_name_train, 'r', encoding='utf-8').readlines()\n",
    "        fead_test = open(file_name_test, 'r', encoding='utf-8').readlines()\n",
    "        train_test_read(train_data, fead_train)\n",
    "        train_test_read(test_data, fead_test)\n",
    "\n",
    "    elif key=='write':  \n",
    "        geshj_tra_tes(train_data, \"oftrain\")\n",
    "        file_write(file_name_train, len_train_data, train_data)\n",
    "        geshj_tra_tes(test_data, \"oftesta\")\n",
    "        file_write(file_name_test, len_test_data, test_data)\n",
    "\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "def list_to_string(alist):\n",
    "    string = ''\n",
    "    for i in alist:\n",
    "        string += i + '#'\n",
    "    return string\n",
    "\n",
    "def file_write(filename, len_data, data):\n",
    "    f = open(filename, 'w', encoding = 'utf-8')\n",
    "    for index in range(len_data):\n",
    "        strings = list_to_string(data.examples[index].text)\n",
    "        f.write(strings + '\\n')\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999\n",
      "1999\n",
      "2999\n",
      "3999\n",
      "4999\n",
      "5999\n",
      "6999\n",
      "7999\n",
      "8999\n",
      "9999\n",
      "10999\n",
      "11999\n",
      "12999\n",
      "13999\n",
      "14999\n",
      "15999\n",
      "16999\n",
      "17999\n",
      "18999\n",
      "19999\n",
      "20999\n",
      "21999\n",
      "22999\n",
      "23999\n",
      "24999\n",
      "999\n",
      "1999\n",
      "2999\n",
      "3999\n",
      "4999\n",
      "5999\n",
      "6999\n",
      "7999\n",
      "8999\n",
      "9999\n",
      "10999\n",
      "11999\n",
      "12999\n",
      "13999\n",
      "14999\n",
      "15999\n",
      "16999\n",
      "17999\n",
      "18999\n",
      "19999\n",
      "20999\n",
      "21999\n",
      "22999\n",
      "23999\n",
      "24999\n"
     ]
    }
   ],
   "source": [
    "read_or_write(key_read_write, proof_number)\n",
    "\n",
    "if raise_Exception and (key_read_write == 'write') :\n",
    "    raise Exception(\"Terminated at this point\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = list(range(len(train_data.examples)))\n",
    "\n",
    "k = args.K \n",
    "random_seed = 42  \n",
    "kfold = KFold(n_splits=k, shuffle=True, random_state=random_seed)\n",
    "\n",
    "train_subsets = []\n",
    "for _, train_idx in kfold.split(train_indices):\n",
    "    train_subset = data.Dataset([train_data.examples[i] for i in train_idx], fields=[('text', TEXT), ('label', LABEL)])\n",
    "    train_subsets.append(train_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_indices = list(range(len(test_data.examples)))\n",
    "\n",
    "random_seed = 24  \n",
    "kfold_te = KFold(n_splits=k, shuffle=True, random_state=random_seed)\n",
    "\n",
    "test_subsets = []\n",
    "for _, test_idx in kfold_te.split(test_indices):\n",
    "    test_subset = data.Dataset([test_data.examples[i] for i in test_idx], fields=[('text', TEXT), ('label', LABEL)])\n",
    "    test_subsets.append(test_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterator(tri_data, tes_data):\n",
    "    \n",
    "    train_iterator, test_iterator = data.BucketIterator.splits(\n",
    "        (tri_data, tes_data),\n",
    "        batch_size = args.B,\n",
    "        sort_key = lambda x: len(x.text),\n",
    "        sort_within_batch=True,\n",
    "        device=torch.device(\"cpu\") \n",
    "    )\n",
    "\n",
    "    return train_iterator, test_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all_sub = []\n",
    "testa_all_sub = []\n",
    "\n",
    "for index in range(len(train_subsets)) :\n",
    "    data_i, data_j = train_subsets[index ], test_subsets[index ]\n",
    "    data_i_reu, data_j_reu = iterator(data_i, data_j)\n",
    "    train_all_sub.append(data_i_reu)\n",
    "    testa_all_sub.append(data_j_reu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.BCEWithLogitsLoss().to(args.device) \n",
    "loss_function_gradient = nn.BCEWithLogitsLoss().to(args.device) \n",
    "\n",
    "def get_val_loss(model, Vall):  \n",
    "    model = model.to(args.device)\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    with torch.no_grad():\n",
    "        for seqq in Vall:\n",
    "            text = seqq.text.to(args.device)\n",
    "            target = seqq.label.to(args.device)\n",
    "            y_preda = model(text).squeeze(1)\n",
    "            loss = loss_function(y_preda, target)\n",
    "            val_loss.append(loss.item())\n",
    "    model = model.to('cpu')        \n",
    "    loss_v = np.array(val_loss).mean()\n",
    "    return loss_v \n",
    "\n",
    "def binary_acc(preds, y):\n",
    "    preds = preds.to('cpu')\n",
    "    y = y.to('cpu')\n",
    "    preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = torch.eq(preds, y).float()\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "def test(args, model, k):\n",
    "    model.eval()\n",
    "    Val = testa_all_sub[k] \n",
    "    avg_acc = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for seq in Val:\n",
    "            text = seq.text.to(args.device)\n",
    "            y_pred = model(text).squeeze(1) \n",
    "            y_pred = y_pred.to('cpu')\n",
    "            target = seq.label.to('cpu')\n",
    "            acc = binary_acc(y_pred, target).item()\n",
    "            avg_acc.append(acc)\n",
    "    avg_acc = np.mean(avg_acc)\n",
    "    \n",
    "    print(\"客户端：\", k, ' avg acc', avg_acc)\n",
    "\n",
    "    wstring = f\"客户端：{k} avg acc: {avg_acc}\"\n",
    "    writecon = str(wstring) + \"\\n\"\n",
    "    file_write_log.write(writecon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectsampling(x):\n",
    "    custom_min = 0.0001\n",
    "    custom_max = 0.9999\n",
    "    scaled_data = (x.max() - x) / (x.max() - x.min()) * (custom_max - custom_min) + custom_min\n",
    "    probs = F.softmax(scaled_data, dim=0)\n",
    "    return probs    \n",
    "\n",
    "def random_selectsampling_batch(Len):\n",
    "    return torch.full([Len], 1.0 / Len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yangbenshaixuan_batch(Dtr, norms, sampling_rate, keynumber0816, Len):\n",
    "    if keynumber0816 == 1:\n",
    "        pi = selectsampling(norms)\n",
    "    elif keynumber0816 == 4:\n",
    "        pi = random_selectsampling_batch(Len)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    num_samples_num = int(sampling_rate * len(pi))\n",
    "\n",
    "    try:\n",
    "        samples = torch.multinomial(pi, num_samples= num_samples_num, replacement=False)\n",
    "    except:\n",
    "        print(\"---except---\")\n",
    "        pi = random_selectsampling_batch(Len)\n",
    "        samples = torch.multinomial(pi, num_samples= num_samples_num, replacement=False)\n",
    "        \n",
    "    new_sample_batch = []\n",
    "    samples = samples.tolist()\n",
    "    iiindex_list = []\n",
    "    for iiindex, batch in enumerate(Dtr):\n",
    "        if iiindex in samples:\n",
    "            new_sample_batch.append(batch)\n",
    "            iiindex_list.append(iiindex)\n",
    "\n",
    "    wstring = f\"minigrid sampling batch:  {iiindex_list} \"\n",
    "    writecon = str(wstring) + \"\\n\"\n",
    "    file_write_log.write(writecon)\n",
    "    \n",
    "    return new_sample_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_select_batch(Dtr, gradient_model, optimizer_gradient):\n",
    "    result = []\n",
    "    for ttime, x_data in enumerate(Dtr):\n",
    "        text = x_data.text.to(args.device)\n",
    "        target = x_data.label.to(args.device)\n",
    "        y_pred = gradient_model(text).squeeze(1)\n",
    "        loss = loss_function_gradient(y_pred, target)\n",
    "        loss.backward(retain_graph=True)\n",
    "        grad_vars = []\n",
    "        for param in gradient_model.parameters():\n",
    "            if param.grad is not None:\n",
    "                grad_vars.append(param.grad.cpu().detach().clone())\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        for param in gradient_model.parameters():\n",
    "            if param.grad is not None:\n",
    "                a_grad = param.grad.detach_()\n",
    "                param.grad.zero_()\n",
    "        c = torch.cat([param.view(-1) for param in grad_vars]).unsqueeze(0).to(\"cpu\")\n",
    "        result.append(c)  \n",
    "\n",
    "    result = torch.cat(result, dim=0)\n",
    "    myresult = result.to('cpu')\n",
    "    norms = torch.norm(myresult, dim=1)\n",
    "    return  norms  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(args, model, k, gradient_model): \n",
    "    Dtr = train_all_sub[k]   \n",
    "    Val = testa_all_sub[k]\n",
    "    \n",
    "    count = 0\n",
    "    for _ in Dtr:\n",
    "        count += 1\n",
    "\n",
    "    if args.optimizer == 'adam': \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr,\n",
    "                                     weight_decay=args.weight_decay)  \n",
    "        optimizer_gradient = torch.optim.Adam(gradient_model.parameters(), lr=args.lr,\n",
    "                                     weight_decay=args.weight_decay) \n",
    "    else:\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=args.lr,\n",
    "                                    momentum=0.9, weight_decay=args.weight_decay)\n",
    "        optimizer_gradient = torch.optim.SGD(gradient_model.parameters(), lr=args.lr,\n",
    "                                    momentum=0.9, weight_decay=args.weight_decay)\n",
    "\n",
    "    min_epochs = 1\n",
    "    best_model = None\n",
    "    min_val_loss = 10000.0\n",
    "    \n",
    "    for epoch in range(args.E):  \n",
    "        train_loss = [] \n",
    "        if keynumber0816 == 1:\n",
    "            gradient_model.load_state_dict(model.state_dict())\n",
    "            gradient_model.train()\n",
    "            gradient_model = gradient_model.to(args.device)\n",
    "            norms = real_select_batch(Dtr, gradient_model, optimizer_gradient)\n",
    "            gradient_model = gradient_model.to('cpu')\n",
    "            batch_list = yangbenshaixuan_batch(Dtr, norms , args.C, keynumber0816, count) \n",
    "\n",
    "        elif keynumber0816 == 4:\n",
    "            batch_list = yangbenshaixuan_batch(Dtr, None, args.batch_sample_rate, keynumber0816, count)\n",
    "\n",
    "        model = model.to(args.device)\n",
    "        for _ in range(2): \n",
    "            for  X_train in  batch_list: \n",
    "                text = X_train.text.to(args.device)  \n",
    "                target = X_train.label.to(args.device)\n",
    "                model.train()\n",
    "                y_pred = model(text).squeeze(1)\n",
    "                loss = loss_function(y_pred, target)\n",
    "                train_loss.append(loss.item())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "        model = model.to('cpu')    \n",
    "\n",
    "        gradient_model.load_state_dict(model.state_dict())\n",
    "        val_loss = get_val_loss(model, Val)\n",
    "\n",
    "        if (file_args_key == \"shake\") and ((epoch+1) % 1 == 0) :\n",
    "            wstring = f\"epoch {epoch} 训练 train_loss {np.array(train_loss).mean()} 验证 val_loss {val_loss}\"\n",
    "            writecon = str(wstring) + \"\\n\"\n",
    "            file_write_log.write(writecon)\n",
    "\n",
    "        if (file_args_key == \"live\") and ((epoch+1) % 1 == 0):\n",
    "            wstring = f\"epoch {epoch} 训练 train_loss {np.array(train_loss).mean()} 验证 val_loss {val_loss}\"\n",
    "            writecon = str(wstring) + \"\\n\"\n",
    "            file_write_log.write(writecon)\n",
    "\n",
    "        if (epoch + 1 >= min_epochs) and (val_loss <= min_val_loss) : \n",
    "            min_val_loss = val_loss\n",
    "        \n",
    "    model = model.to('cpu')\n",
    "    avg_val_lossa = np.array(train_loss).mean()\n",
    "\n",
    "    wstring = f\"model {k} val_loss: {avg_val_lossa} \"\n",
    "    writecon = str(wstring) + \"\\n\"\n",
    "    file_write_log.write(writecon)\n",
    "\n",
    "    wstring = f\"训练 best min train_loss {min_val_loss} \"\n",
    "    writecon = str(wstring) + \"\\n\"\n",
    "    file_write_log.write(writecon)\n",
    "    print('model {}, avg_loss {:.8f},  min_loss {:.8f}, val loss {:.8f} '.format(k, avg_val_lossa,min_val_loss, val_loss ))\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FedAvg:\n",
    "\n",
    "    def __init__(self, args):\n",
    "\n",
    "        self.args = args \n",
    "        self.clients = args.clients  \n",
    "        \n",
    "        if yesorno_inputdim == 50:\n",
    "            self.nn = RNN50(vocab_size, args, name='server').to('cpu') \n",
    "            self.nn.embedding.weight.data.copy_(pretrained_embedding)\n",
    "            \n",
    "            self.nn_gradient = RNN50_gradient(vocab_size, args, name='nn_gradient').to('cpu')\n",
    "            self.nn_gradient.embedding.weight.data.copy_(pretrained_embedding)\n",
    "\n",
    "        elif (proof_number == 1) or (proof_number == 3) or (proof_number == 6):\n",
    "            self.nn = RNN50(vocab_size, args, name='server').to('cpu')   # \n",
    "            self.nn.embedding.weight.data.copy_(new_pretrained_embedding)\n",
    "            \n",
    "\n",
    "            self.nn_gradient = RNN50_gradient(vocab_size, args, name='nn_gradient').to('cpu')  \n",
    "            self.nn_gradient.embedding.weight.data.copy_(new_pretrained_embedding)\n",
    "        \n",
    "        elif (proof_number == 0) or (proof_number == 2) :\n",
    "            self.nn = RNN(vocab_size, args, name='server').to('cpu')\n",
    "            self.nn.embedding.weight.data.copy_(pretrained_embedding)\n",
    "\n",
    "            self.nn_gradient = RNN_gradient(vocab_size, args, name='nn_gradient').to('cpu')\n",
    "            self.nn_gradient.embedding.weight.data.copy_(pretrained_embedding)\n",
    "\n",
    "        self.nns = []\n",
    "        self.m = 0\n",
    "        \n",
    "        for i in range(self.args.K):    \n",
    "            temp = copy.deepcopy(self.nn)\n",
    "            temp.name = self.clients[i]  \n",
    "            self.nns.append(temp)  \n",
    "    \n",
    "    def server(self):\n",
    "        for t in range(self.args.r):  \n",
    "            print('round：--------', t + 1, '------------------------')\n",
    "            wstring = f\"round：-------- {t + 1} ------------------------\"\n",
    "            writecon = str(wstring) + \"\\n\"\n",
    "            file_write_log.write(writecon)\n",
    "            m = max(int(self.args.C * self.args.K), 1)\n",
    "            index = torch.randperm(self.args.K)[:m].tolist()\n",
    "            self.m = m\n",
    "            print('选取客户端:',index)\n",
    "\n",
    "            wstring = f\"选取客户端: {index} \"\n",
    "            writecon = str(wstring) + \"\\n\"\n",
    "            file_write_log.write(writecon)\n",
    "            self.dispatch(index) \n",
    "            self.client_update(index)  \n",
    "            self.aggregation(index) \n",
    "\n",
    "        torch.save(self.nn.state_dict(), save_name_pth)\n",
    "        print(save_name_pth, 'save')\n",
    "        return self.nn\n",
    "\n",
    "    def dispatch(self, index): \n",
    "        self.nn = self.nn.to('cpu')\n",
    "        \n",
    "        for j in index:\n",
    "            self.nns[j] = self.nns[j].to('cpu')\n",
    "            for old_params, new_params in zip(self.nns[j].parameters(), self.nn.parameters()):\n",
    "                old_params.data = new_params.data.clone()\n",
    "            self.nns[j] = self.nns[j].to('cpu')\n",
    "        self.nn = self.nn.to('cpu')\n",
    "\n",
    "    def client_update(self, index):  \n",
    "        if batch_sanpling_bur:\n",
    "            for k in index: \n",
    "                wstring = f\"训练本地模型： {k} \"\n",
    "                writecon = str(wstring) + \"\\n\"\n",
    "                file_write_log.write(writecon)\n",
    "                self.nns[k] = self.nns[k].to('cpu')\n",
    "                self.nns[k] = train_batch(self.args, self.nns[k], k, self.nn_gradient)  \n",
    "\n",
    "\n",
    "    def aggregation(self, index): \n",
    "        s = 0\n",
    "        params = {}\n",
    "        self.nns[0] = self.nns[0].to('cpu')\n",
    "        for k, v in self.nns[0].named_parameters(): \n",
    "            params[k] = torch.zeros_like(v.data)\n",
    "\n",
    "        for j in index:\n",
    "            self.nns[j] = self.nns[j].to('cpu')\n",
    "            for k, v in self.nns[j].named_parameters():  \n",
    "                params[k] += v.data * (1.0 / self.m )  \n",
    "\n",
    "        for k, v in self.nn.named_parameters():\n",
    "            v.data = params[k].data.clone() \n",
    "        \n",
    "\n",
    "    def global_test(self, args): \n",
    "        model = self.nn.to(args.device) \n",
    "        c = args.clients \n",
    "        for k, client in enumerate(c):\n",
    "            test(self.args, model, k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round：-------- 1 ------------------------\n",
      "选取客户端: [17, 11, 15, 14, 5, 7, 1, 4, 3, 6]\n",
      "model 17, avg_loss 0.58085688,  min_loss 0.64114106, val loss 0.72863675 \n",
      "model 11, avg_loss 0.28586414,  min_loss 0.62624025, val loss 0.69952258 \n",
      "model 15, avg_loss 0.54937039,  min_loss 0.58856579, val loss 0.68944137 \n",
      "model 14, avg_loss 0.38687204,  min_loss 0.58649035, val loss 0.65713187 \n",
      "model 5, avg_loss 0.47773856,  min_loss 0.64138949, val loss 0.64692744 \n",
      "model 7, avg_loss 0.22590632,  min_loss 0.60284530, val loss 0.76843810 \n",
      "model 1, avg_loss 0.31950964,  min_loss 0.59302849, val loss 0.65774514 \n",
      "model 4, avg_loss 0.57485422,  min_loss 0.57167466, val loss 0.69597919 \n",
      "model 3, avg_loss 0.64075876,  min_loss 0.59781529, val loss 0.62355149 \n",
      "model 6, avg_loss 0.32735344,  min_loss 0.64118619, val loss 0.70435146 \n",
      "round：-------- 2 ------------------------\n",
      "选取客户端: [14, 17, 16, 4, 5, 19, 0, 15, 6, 8]\n",
      "model 14, avg_loss 0.22037864,  min_loss 0.56418565, val loss 0.82905021 \n",
      "model 17, avg_loss 0.40540556,  min_loss 0.53050278, val loss 0.67694832 \n",
      "model 16, avg_loss 0.37060711,  min_loss 0.56776642, val loss 0.66368464 \n",
      "model 4, avg_loss 0.54558674,  min_loss 0.61138818, val loss 0.69346502 \n",
      "model 5, avg_loss 0.29616341,  min_loss 0.49738403, val loss 0.79078224 \n",
      "model 19, avg_loss 0.16106424,  min_loss 0.51570902, val loss 0.68753924 \n",
      "model 0, avg_loss 0.23647523,  min_loss 0.58956353, val loss 0.66017687 \n",
      "model 15, avg_loss 0.22390360,  min_loss 0.56635418, val loss 0.64305348 \n",
      "model 6, avg_loss 0.19711289,  min_loss 0.53152701, val loss 0.85690332 \n",
      "model 8, avg_loss 0.22938890,  min_loss 0.52975969, val loss 0.70595076 \n",
      "round：-------- 3 ------------------------\n",
      "选取客户端: [15, 3, 17, 12, 5, 14, 8, 1, 9, 19]\n",
      "model 15, avg_loss 0.16760339,  min_loss 0.47299637, val loss 0.59802606 \n",
      "model 3, avg_loss 0.14861315,  min_loss 0.52931224, val loss 0.85518463 \n",
      "model 17, avg_loss 0.07210661,  min_loss 0.49815651, val loss 0.85809527 \n",
      "model 12, avg_loss 0.15732830,  min_loss 0.47202749, val loss 0.68220567 \n",
      "model 5, avg_loss 0.14902719,  min_loss 0.46554791, val loss 0.82881439 \n",
      "model 14, avg_loss 0.15970806,  min_loss 0.48850511, val loss 0.76388504 \n",
      "model 8, avg_loss 0.14005680,  min_loss 0.50234679, val loss 0.59436700 \n",
      "model 1, avg_loss 0.13601221,  min_loss 0.49404637, val loss 0.89900662 \n",
      "model 9, avg_loss 0.47191371,  min_loss 0.45909309, val loss 0.62184637 \n",
      "model 19, avg_loss 0.11091351,  min_loss 0.47157635, val loss 0.76581104 \n",
      "round：-------- 4 ------------------------\n",
      "选取客户端: [4, 15, 16, 3, 10, 14, 2, 0, 8, 7]\n",
      "model 4, avg_loss 0.11288658,  min_loss 0.48616312, val loss 0.86648766 \n",
      "model 15, avg_loss 0.04935492,  min_loss 0.48273281, val loss 0.83881804 \n",
      "model 16, avg_loss 0.07478919,  min_loss 0.46192026, val loss 0.91040783 \n",
      "model 3, avg_loss 0.03772485,  min_loss 0.45953324, val loss 0.91084048 \n",
      "model 10, avg_loss 0.11417734,  min_loss 0.45972392, val loss 0.65287576 \n",
      "model 14, avg_loss 0.08395184,  min_loss 0.47238858, val loss 0.71246178 \n",
      "model 2, avg_loss 0.05716005,  min_loss 0.44624593, val loss 0.85642261 \n",
      "model 0, avg_loss 0.59940200,  min_loss 0.47868485, val loss 0.76245810 \n",
      "model 8, avg_loss 0.03840473,  min_loss 0.42871519, val loss 1.03266096 \n",
      "model 7, avg_loss 0.05814088,  min_loss 0.45312417, val loss 0.77499403 \n",
      "round：-------- 5 ------------------------\n",
      "选取客户端: [16, 0, 3, 13, 17, 18, 10, 8, 2, 7]\n",
      "model 16, avg_loss 0.02344864,  min_loss 0.43113901, val loss 1.10366565 \n",
      "model 0, avg_loss 0.02761870,  min_loss 0.42130336, val loss 0.73494724 \n",
      "model 3, avg_loss 0.04034896,  min_loss 0.46255178, val loss 0.84032251 \n",
      "model 13, avg_loss 0.04753761,  min_loss 0.37154968, val loss 1.21146654 \n",
      "model 17, avg_loss 0.01835710,  min_loss 0.44386667, val loss 0.94812697 \n",
      "model 18, avg_loss 0.03261192,  min_loss 0.44660264, val loss 0.80563153 \n",
      "model 10, avg_loss 0.04098980,  min_loss 0.37480169, val loss 1.18371755 \n",
      "model 8, avg_loss 0.03460520,  min_loss 0.40169405, val loss 0.71227960 \n",
      "model 2, avg_loss 0.03397419,  min_loss 0.44444867, val loss 0.67260196 \n",
      "model 7, avg_loss 0.03953405,  min_loss 0.43234399, val loss 0.66396733 \n",
      "round：-------- 6 ------------------------\n",
      "选取客户端: [2, 5, 13, 7, 10, 11, 3, 6, 4, 8]\n",
      "model 2, avg_loss 0.01227461,  min_loss 0.42005902, val loss 0.95791110 \n",
      "model 5, avg_loss 0.00715660,  min_loss 0.37431639, val loss 1.09319369 \n",
      "model 13, avg_loss 0.01617416,  min_loss 0.36912965, val loss 0.70645150 \n",
      "model 7, avg_loss 0.01848820,  min_loss 0.45559666, val loss 0.98283403 \n",
      "model 10, avg_loss 0.02152131,  min_loss 0.39081715, val loss 0.88005741 \n",
      "model 11, avg_loss 0.02025397,  min_loss 0.41434375, val loss 0.77642386 \n",
      "model 3, avg_loss 0.06800788,  min_loss 0.46413186, val loss 0.73644171 \n",
      "model 6, avg_loss 0.00613912,  min_loss 0.38771356, val loss 0.83595738 \n",
      "model 4, avg_loss 0.02058669,  min_loss 0.47392449, val loss 0.96413262 \n",
      "model 8, avg_loss 0.05182170,  min_loss 0.40346740, val loss 1.00259567 \n",
      "round：-------- 7 ------------------------\n",
      "选取客户端: [7, 0, 12, 13, 14, 16, 6, 19, 18, 15]\n",
      "model 7, avg_loss 0.06919864,  min_loss 0.42562500, val loss 1.05493821 \n",
      "model 0, avg_loss 0.02399266,  min_loss 0.38370401, val loss 1.23605151 \n",
      "model 12, avg_loss 0.00399038,  min_loss 0.34513566, val loss 0.79120305 \n",
      "model 13, avg_loss 0.01357043,  min_loss 0.40240711, val loss 0.94466476 \n",
      "model 14, avg_loss 0.02361534,  min_loss 0.40572655, val loss 1.05206396 \n",
      "model 16, avg_loss 0.03076708,  min_loss 0.39951538, val loss 1.14357841 \n",
      "model 6, avg_loss 0.01940539,  min_loss 0.35935388, val loss 0.76277102 \n",
      "model 19, avg_loss 0.02685740,  min_loss 0.33450996, val loss 0.91001876 \n",
      "model 18, avg_loss 0.04626033,  min_loss 0.40247657, val loss 0.89387969 \n",
      "model 15, avg_loss 0.04956836,  min_loss 0.35909322, val loss 0.84384276 \n",
      "round：-------- 8 ------------------------\n",
      "选取客户端: [6, 1, 15, 2, 5, 8, 11, 12, 17, 19]\n",
      "model 6, avg_loss 0.01123867,  min_loss 0.37263245, val loss 0.94700625 \n",
      "model 1, avg_loss 0.01060897,  min_loss 0.37864482, val loss 0.87214616 \n",
      "model 15, avg_loss 0.01017405,  min_loss 0.41237071, val loss 0.96896520 \n",
      "model 2, avg_loss 0.02169013,  min_loss 0.40487072, val loss 0.87616910 \n",
      "model 5, avg_loss 0.01639978,  min_loss 0.37429002, val loss 0.97637497 \n",
      "model 8, avg_loss 0.01793998,  min_loss 0.40035306, val loss 0.70997762 \n",
      "model 11, avg_loss 0.00354899,  min_loss 0.38851938, val loss 0.92738185 \n",
      "model 12, avg_loss 0.05690042,  min_loss 0.36228725, val loss 0.98650352 \n",
      "model 17, avg_loss 0.00760043,  min_loss 0.37503536, val loss 1.06893712 \n",
      "model 19, avg_loss 0.01132536,  min_loss 0.43948058, val loss 0.96430770 \n",
      "round：-------- 9 ------------------------\n",
      "选取客户端: [11, 16, 8, 10, 4, 19, 6, 7, 13, 14]\n",
      "model 11, avg_loss 0.00290730,  min_loss 0.41892839, val loss 0.87480472 \n",
      "model 16, avg_loss 0.02812343,  min_loss 0.36596603, val loss 0.84978812 \n",
      "model 8, avg_loss 0.56637567,  min_loss 0.45740088, val loss 0.65244850 \n",
      "model 10, avg_loss 0.00716616,  min_loss 0.35117480, val loss 1.00726718 \n",
      "model 4, avg_loss 0.00299389,  min_loss 0.40450464, val loss 1.26464628 \n",
      "model 19, avg_loss 0.00295335,  min_loss 0.36784968, val loss 0.81069504 \n",
      "model 6, avg_loss 0.01900001,  min_loss 0.35344238, val loss 0.92657484 \n",
      "model 7, avg_loss 0.00469166,  min_loss 0.39678796, val loss 1.34315021 \n",
      "model 13, avg_loss 0.03068696,  min_loss 0.36500591, val loss 0.93649597 \n",
      "model 14, avg_loss 0.00834728,  min_loss 0.43564652, val loss 0.98451834 \n",
      "round：-------- 10 ------------------------\n",
      "选取客户端: [3, 15, 4, 17, 14, 9, 2, 5, 10, 0]\n",
      "model 3, avg_loss 0.00109371,  min_loss 0.43999359, val loss 1.03843738 \n",
      "model 15, avg_loss 0.06812334,  min_loss 0.41590616, val loss 0.94230938 \n",
      "model 4, avg_loss 0.05112435,  min_loss 0.41962431, val loss 1.50907288 \n",
      "model 17, avg_loss 0.02119976,  min_loss 0.39198023, val loss 1.06057150 \n",
      "model 14, avg_loss 0.00958364,  min_loss 0.40684284, val loss 0.99698856 \n",
      "model 9, avg_loss 0.00725311,  min_loss 0.36440675, val loss 0.90067465 \n",
      "model 2, avg_loss 0.00584834,  min_loss 0.41606221, val loss 1.10338151 \n",
      "model 5, avg_loss 0.00138655,  min_loss 0.39290525, val loss 0.87333259 \n",
      "model 10, avg_loss 0.01573090,  min_loss 0.40245844, val loss 0.83593644 \n",
      "model 0, avg_loss 0.00578931,  min_loss 0.38858491, val loss 0.96599283 \n",
      "/mnt/workspace/fedavg_ceshi/out_pth/live_0_4_0_30_0.5_write_08_30_2023_06_19_08_dict.pth save\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RNN50(\n",
       "  (embedding): Embedding(10002, 50)\n",
       "  (rnn): LSTM(50, 256, num_layers=2, dropout=0.5, bidirectional=True)\n",
       "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fed = FedAvg(args)\n",
    "fed.server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "客户端： 0  avg acc 0.8380952292964572\n",
      "客户端： 1  avg acc 0.854365070660909\n",
      "客户端： 2  avg acc 0.83888888217154\n",
      "客户端： 3  avg acc 0.8392857086090815\n",
      "客户端： 4  avg acc 0.8333333304950169\n",
      "客户端： 5  avg acc 0.847619042510078\n",
      "客户端： 6  avg acc 0.85952380441484\n",
      "客户端： 7  avg acc 0.8388888850098565\n",
      "客户端： 8  avg acc 0.8503968218962351\n",
      "客户端： 9  avg acc 0.867857134058362\n",
      "客户端： 10  avg acc 0.8559523749919165\n",
      "客户端： 11  avg acc 0.8416666629768553\n",
      "客户端： 12  avg acc 0.8511904662563687\n",
      "客户端： 13  avg acc 0.8511904690946851\n",
      "客户端： 14  avg acc 0.8424603059178307\n",
      "客户端： 15  avg acc 0.8424603130136218\n",
      "客户端： 16  avg acc 0.828174596741086\n",
      "客户端： 17  avg acc 0.8329364998000008\n",
      "客户端： 18  avg acc 0.8408730101017725\n",
      "客户端： 19  avg acc 0.859920628014065\n"
     ]
    }
   ],
   "source": [
    "fed.global_test(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_write_log.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e3f128cad0f25ad952b91777a639b4fa2222dfd0378c44e8bdbd611468a92202"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
