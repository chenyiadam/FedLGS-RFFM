{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_args_key = \"live\"\n",
    "proof_number = 6\n",
    "keynumber0816 = 4\n",
    "kkkkkkkkkkkk = 0  \n",
    "key_read_write = 'write'  \n",
    "raise_Exception = False \n",
    "batch_sanpling_bur = True  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.0+cu113\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext.legacy import data, datasets\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "import copy\n",
    "# from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# from model_nn import RNN, RNN_gradient,RNN50, RNN50_gradient\n",
    "# from args_fedavg import args_parser_live, args_parser_shake\n",
    "from model_nn_min import RNN, RNN_gradient,RNN50, RNN50_gradient\n",
    "from args_fvg import args_parser_live, args_parser_shake\n",
    "\n",
    "from nltk.tag import pos_tag\n",
    "import re\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/workspace/fedavg_ceshi/out_log/log_live_6_4_0_30_0.5_write_08_27_2023_11_08_42_log.txt\n",
      "/mnt/workspace/fedavg_ceshi/out_pth/live_6_4_0_30_0.5_write_08_27_2023_11_08_42_dict.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_log = r\"/mnt/workspace/fedavg_ceshi/out_log\"\n",
    "out_pth = r\"/mnt/workspace/fedavg_ceshi/out_pth\"\n",
    "write_read_txt = r\"/mnt/workspace/fedavg_ceshi/write_read_txt\"\n",
    "\n",
    "if file_args_key == \"shake\":\n",
    "    args = args_parser_shake()\n",
    "elif file_args_key == \"live\":\n",
    "    args = args_parser_live()\n",
    "\n",
    "String = time.strftime(\"%m_%d_%Y_%H_%M_%S\",time.localtime())\n",
    "namestr = str(String)\n",
    "\n",
    "file_write_log_name = out_log + \"/\" + \"log_\" + file_args_key + '_'  + str(proof_number) \\\n",
    "    + '_'+ str(keynumber0816) + '_'+ str(kkkkkkkkkkkk) + '_' \\\n",
    "        + str(args.B) + '_'+ str(args.C) + '_' + key_read_write + '_'+ namestr + \"_log.txt\"\n",
    "\n",
    "print(file_write_log_name)\n",
    "\n",
    "file_write_log = open(file_write_log_name, \"w\", encoding='utf-8')\n",
    "\n",
    "save_name_pth = out_pth + \"/\" +  file_args_key + '_'  + str(proof_number) \\\n",
    "    + '_'+ str(keynumber0816) + '_'+ str(kkkkkkkkkkkk) + '_' \\\n",
    "        + str(args.B) + '_'+ str(args.C) + '_' + key_read_write + '_'+ namestr +'_dict.pth'\n",
    "\n",
    "print(save_name_pth)\n",
    "\n",
    "writecon = str(save_name_pth) + \"\\n\"\n",
    "file_write_log.write(writecon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of train data: 25000\n",
      "len of test data: 25000\n",
      "['I', 'was', 'five', 'when', 'the', 'show', 'made', 'its', 'debut', 'in', '1958', 'and', 'at', 'a', 'later', 'point', ',', 'was', 'a', 'regular', 'viewer', '.', 'I', 'remember', 'that', 'I', 'really', 'enjoyed', 'the', 'show', ',', 'along', 'with', '\"', 'Leave', 'It', 'To', 'Beaver', '\"', ',', '\"', 'My', 'Three', 'Sons', '\"', ',', '\"', 'Ozzie', 'and', 'Harriet', '\"', ',', '\"', 'Dick', 'Van', 'Dyke', '\"', ',', 'reruns', 'of', '\"', 'I', 'Love', 'Lucy', '\"', ',', '\"', 'The', 'Real', 'McCoys', '\"', ',', 'etc', '.', 'I', 'am', 'now', 'enjoying', 'the', 'first', 'season', 'of', '\"', 'Donna', 'Reed', '\"', 'on', 'DVD', 'and', 'have', 'watched', 'the', 'first', 'two', 'episodes', '.', 'Donna', 'Stone', 'is', 'shown', 'to', 'be', 'an', 'intelligent', ',', 'well', '-', 'mannered', ',', 'problem', '-', 'solving', ',', 'serene', ',', 'stay', '-', 'at', '-', 'home', 'mom', ',', 'similar', 'to', 'June', 'Cleaver', 'and', 'in', 'contrast', 'to', 'Lucy', 'Ricardo', '.', 'In', 'episode', '2', ',', 'I', 'especially', 'like', 'how', 'Ms.', 'Reed', 'becomes', 'a', 'surrogate', 'dad', ',', 'trading', 'in', 'her', 'dress', 'for', 'sweats', 'and', 'boxing', 'gloves', ',', 'while', 'teaching', 'her', 'son', 'how', 'to', 'defend', 'himself', 'physically', 'against', 'a', 'much', 'larger', 'bully', '.', 'While', 'none', 'of', 'the', 'mothers', 'in', 'the', 'neighborhood', 'I', 'grew', 'up', 'in', ',', 'including', 'my', 'own', ',', 'exactly', 'met', 'the', 'idealistic', 'standards', 'portrayed', 'by', 'Ms.', 'Reed', ',', 'it', 'is', 'refreshing', 'to', 'see', 'good', 'manners', 'and', 'intelligent', 'decision', '-', 'making', 'prevail', 'at', 'the', 'end', 'of', 'the', 'day', ',', 'in', 'contrast', 'to', 'today', \"'s\", 'accepted', 'standards', 'of', 'vulgarity', ',', 'selfishness', 'and', 'indifference', 'among', 'one', \"'s\", 'neighbors', '.', 'I', 'can', 'not', 'imagine', 'Jeff', 'and', 'Mary', 'Stone', 'being', 'told', 'by', 'their', 'parents', 'that', 'trespassing', 'in', 'their', 'neighbors', \"'\", 'yards', 'is', 'okay', ',', 'leaving', 'a', 'dog', 'outside', 'to', 'bark', 'all', 'day', 'is', 'acceptable', ',', 'or', 'telling', 'their', 'mother', 'to', '\"', 'shut', 'up', '\"', 'in', 'a', 'supermarket', 'in', 'front', 'of', 'everyone', '.']\n",
      "pos\n"
     ]
    }
   ],
   "source": [
    "TEXT = data.Field(tokenize='spacy')\n",
    "LABEL = data.LabelField(dtype=torch.float)\n",
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
    "\n",
    "if file_args_key == \"shake\":\n",
    "    # train_data.examples = train_data.examples[:len(train_data.examples)//500]\n",
    "    # test_data.examples = test_data.examples[:len(test_data.examples)//500]\n",
    "\n",
    "    train_data.examples = train_data.examples[:len(train_data.examples)//125]\n",
    "    test_data.examples = test_data.examples[:len(test_data.examples)//125]\n",
    "\n",
    "len_train_data = len(train_data)\n",
    "len_test_data = len(test_data)\n",
    "\n",
    "print('len of train data:', len(train_data))\n",
    "print('len of test data:', len(test_data))\n",
    "print(test_data.examples[1].text) \n",
    "print(test_data.examples[1].label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import nltk\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "a = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10002 torch.Size([10002, 100])\n"
     ]
    }
   ],
   "source": [
    "TEXT.build_vocab(train_data, max_size=10000, vectors='glove.6B.100d')\n",
    "LABEL.build_vocab(train_data)\n",
    "vocab_size =  len(TEXT.vocab)\n",
    "pretrained_embedding = TEXT.vocab.vectors\n",
    "\n",
    "print(vocab_size, pretrained_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RFFM(i, x):\n",
    "    torch.manual_seed(i)\n",
    "    input_dim = 100\n",
    "    output_dim = 50\n",
    "    sigma = 1.0\n",
    "    omega = torch.randn(output_dim, input_dim, device=args.device) * sigma\n",
    "    b = torch.rand(output_dim, device=args.device) * 2 * np.pi\n",
    "    x = x.view(-1, input_dim).to(args.device)\n",
    "    z = torch.cos(torch.mm(x, omega.t()) + b)\n",
    "    Z_Out =  torch.sqrt(2.0 / torch.tensor(output_dim)) * z\n",
    "    return Z_Out.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10002, 50])\n"
     ]
    }
   ],
   "source": [
    "new_vectors = []\n",
    "for i, word in enumerate(TEXT.vocab.itos):\n",
    "    old_vector = pretrained_embedding[i]\n",
    "    new_vector = RFFM(i, old_vector)\n",
    "    new_vectors.append(new_vector)\n",
    "\n",
    "new_pretrained_embedding = torch.stack(new_vectors, dim=0)\n",
    "new_pretrained_embedding = new_pretrained_embedding.reshape(-1,50)\n",
    "print(new_pretrained_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_vector_50(word):\n",
    "    word_index = TEXT.vocab.stoi[word]\n",
    "    word_vector = new_pretrained_embedding[word_index]\n",
    "    return word_vector\n",
    "\n",
    "def proof1(a, b, epcluo):\n",
    "    a = a.to(torch.float)\n",
    "    b = b.to(torch.float)\n",
    "    dot_product = torch.dot(a, b)\n",
    "    norm_a = torch.norm(a, p=2)\n",
    "    norm_b = torch.norm(b, p=2)\n",
    "    similarity = dot_product / (norm_a * norm_b)\n",
    "    return torch.exp(similarity* epcluo / 2.0 *(-1))\n",
    "\n",
    "def proof1_change(word_list,  proper_nouns, filtered, proper_nouns_pos):\n",
    "    if len(proper_nouns) == 0 or len(filtered) == 0 :  \n",
    "        return word_list\n",
    "    epcluo = args.epcluo\n",
    "    gam = []\n",
    "\n",
    "    xlist = [word_vector_50(word) for word in proper_nouns]\n",
    "    ylist = [word_vector_50(word) for word in filtered]\n",
    "    \n",
    "    for x in xlist:\n",
    "        x_list = []\n",
    "        for y in ylist:\n",
    "            res = proof1(x, y, epcluo)\n",
    "            x_list.append(res.item())\n",
    "        gam.append(x_list)\n",
    "\n",
    "    gam_tensor = torch.tensor(gam)\n",
    "    row_sums = torch.sum(gam_tensor, dim=1, keepdim=True)  \n",
    "    result_prob = torch.div(gam_tensor, row_sums) \n",
    "    sample = torch.multinomial(result_prob, 1)\n",
    "    list_result = sample.squeeze().tolist()\n",
    "    try:\n",
    "        list_result = list(list_result)\n",
    "    except:\n",
    "        list_result = [list_result]\n",
    "    rresul = [filtered[i] for i in list_result]\n",
    "    iiindex = 0\n",
    "\n",
    "    for index, word in enumerate(proper_nouns_pos):\n",
    "        if (word[0] in proper_nouns) and (word[1].startswith('NNP') or word[1].startswith('NNPS') ):\n",
    "            newword = rresul[iiindex] \n",
    "            word_list[index] = newword\n",
    "            iiindex += 1   \n",
    "    \n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_vector_100(word):\n",
    "    word_index = TEXT.vocab.stoi[word]\n",
    "    word_vector = TEXT.vocab.vectors[word_index]\n",
    "    return word_vector\n",
    "\n",
    "def proof2(x, y, epcluo):\n",
    "    K = torch.exp(torch.norm(x-y, p=2).pow(2) / 2.0 *(-1) )\n",
    "    result = K * epcluo / 2.0 *(-1) \n",
    "    e14 = torch.exp(result)\n",
    "    return e14\n",
    "\n",
    "def proof2_change(word_list,  proper_nouns, filtered, proper_nouns_pos):\n",
    "    if len(proper_nouns) == 0 or len(filtered) == 0 :  \n",
    "        return word_list\n",
    "    epcluo = args.epcluo\n",
    "    gam = []\n",
    "    xlist = [word_vector_100(word) for word in proper_nouns]\n",
    "    ylist = [word_vector_100(word) for word in filtered]\n",
    "    \n",
    "    for x in xlist:\n",
    "        x_list = []\n",
    "        for y in ylist:\n",
    "            res = proof2(x, y, epcluo)\n",
    "            x_list.append(res.item())\n",
    "        gam.append(x_list)\n",
    "\n",
    "    gam_tensor = torch.tensor(gam)\n",
    "    row_sums = torch.sum(gam_tensor, dim=1, keepdim=True) \n",
    "    result_prob = torch.div(gam_tensor, row_sums)  \n",
    "    sample = torch.multinomial(result_prob, 1)\n",
    "    list_result = sample.squeeze().tolist()\n",
    "\n",
    "    try:\n",
    "        list_result = list(list_result)\n",
    "    except:\n",
    "        list_result = [list_result]\n",
    "    rresul = [filtered[i] for i in list_result]\n",
    "    iiindex = 0\n",
    "\n",
    "    for index, word in enumerate(proper_nouns_pos):\n",
    "        if (word[0] in proper_nouns) and (word[1].startswith('NNP') or word[1].startswith('NNPS') ):\n",
    "            newword = rresul[iiindex]\n",
    "            word_list[index] = newword\n",
    "            iiindex += 1\n",
    "\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proof3(zx, zy, epcluo): \n",
    "    result = (torch.dot(zx, zy)) * epcluo / 4.0 *(-1) \n",
    "    e14 = torch.exp(result)\n",
    "    return e14\n",
    "\n",
    "def proof3_change(word_list,  proper_nouns, filtered, proper_nouns_pos):\n",
    "    if len(proper_nouns) == 0 or len(filtered) == 0 : \n",
    "        return word_list\n",
    "    epcluo = args.epcluo\n",
    "    gam = []\n",
    "    xlist = [word_vector_50(word) for word in proper_nouns]\n",
    "    ylist = [word_vector_50(word) for word in filtered]\n",
    "    \n",
    "    for x in xlist:\n",
    "        x_list = []\n",
    "        for y in ylist:\n",
    "            res = proof3(x, y, epcluo)\n",
    "            x_list.append(res.item())\n",
    "        gam.append(x_list)\n",
    "    gam_tensor = torch.tensor(gam)\n",
    "    row_sums = torch.sum(gam_tensor, dim=1, keepdim=True) \n",
    "    result_prob = torch.div(gam_tensor, row_sums)  \n",
    "    sample = torch.multinomial(result_prob, 1)\n",
    "    list_result = sample.squeeze().tolist()\n",
    "    try:\n",
    "        list_result = list(list_result)\n",
    "    except:\n",
    "        list_result = [list_result]\n",
    "    rresul = [filtered[i] for i in list_result]\n",
    "    iiindex = 0\n",
    "    for index, word in enumerate(proper_nouns_pos):\n",
    "        if (word[0] in proper_nouns) and (word[1].startswith('NNP') or word[1].startswith('NNPS') ):\n",
    "            newword = rresul[iiindex]\n",
    "            word_list[index] = newword\n",
    "            iiindex += 1\n",
    "\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proof6(zx, zy, epcluo):\n",
    "    result = (torch.dot(zx, zy)) * epcluo / 4.0 *(-1) \n",
    "    e14 = torch.exp(result)\n",
    "    return e14\n",
    "\n",
    "def proof6_change(word_list,  proper_nouns, filtered, proper_nouns_pos, filtered_y, proper_nouns_x ):\n",
    "    if len(proper_nouns) == 0 or len(filtered) == 0 :  \n",
    "        return word_list\n",
    "    \n",
    "    epcluo = args.epcluo\n",
    "    gam = []\n",
    "    xlist = [word_vector_50(word) for word in proper_nouns]\n",
    "    ylist = [word_vector_50(word) for word in filtered]\n",
    "    ylist_f = [word_vector_50(word) for word in filtered_y]\n",
    "    xlist_f = [word_vector_50(word) for word in proper_nouns_x]\n",
    "    \n",
    "    for x in xlist:\n",
    "        x_list = []\n",
    "        for y in ylist:\n",
    "            res = proof6(x, y, epcluo)\n",
    "            x_list.append(res.item())\n",
    "        gam.append(x_list)\n",
    "\n",
    "    gam_tensor = torch.tensor(gam)\n",
    "    row_sums = torch.sum(gam_tensor, dim=1, keepdim=True)  \n",
    "    result_prob = torch.div(gam_tensor, row_sums)  \n",
    "    sample = torch.multinomial(result_prob, 1)\n",
    "    tensor = sample.clone()\n",
    "    list_result = tensor.squeeze().tolist()\n",
    "    try:\n",
    "        list_result = list(list_result)\n",
    "    except:\n",
    "        list_result = [list_result]\n",
    "    rresul = [filtered[i] for i in list_result]\n",
    "    pr = args.pr\n",
    "    gam_pr = []\n",
    "\n",
    "    for y in ylist_f:\n",
    "        x_list_new = []\n",
    "        for x in xlist_f:\n",
    "            res = proof6(y, x, epcluo)\n",
    "            x_list_new.append(res.item())\n",
    "        gam_pr.append(x_list_new)\n",
    "\n",
    "    gam_pr_tensor = torch.tensor(gam_pr)\n",
    "    row_pr_sums = torch.sum(gam_pr_tensor, dim=1, keepdim=True)  \n",
    "    result_prob = pr * torch.div(gam_pr_tensor, row_pr_sums)  \n",
    "    column_to_add = torch.full((result_prob.shape[0], 1), pr)\n",
    "    tensor_with_column = torch.cat((result_prob, column_to_add), dim=1)\n",
    "    sample_pr = torch.multinomial(tensor_with_column, 1)\n",
    "    tensor_pr = sample_pr.clone()\n",
    "    list_result_pr = tensor_pr.squeeze().tolist()\n",
    "\n",
    "    try:\n",
    "        list_result_pr = list(list_result_pr)\n",
    "    except:\n",
    "        list_result_pr = [list_result_pr]\n",
    "\n",
    "    rresuly = []\n",
    "    xylong = len(tensor_with_column[0,:]) - 1\n",
    "\n",
    "    for index, yword in enumerate(list_result_pr):\n",
    "        if yword == xylong :\n",
    "            rresuly.append(filtered_y[index])\n",
    "        else:\n",
    "            rresuly.append(proper_nouns_x[yword])\n",
    "    iiindex = 0\n",
    "    iiindey = 0\n",
    "    \n",
    "    for index, word in enumerate(proper_nouns_pos):\n",
    "        if (word[0] in proper_nouns) and (word[1].startswith('NNP') or word[1].startswith('NNPS') ):\n",
    "            newword = rresul[iiindex]\n",
    "            word_list[index] = newword\n",
    "            iiindex += 1\n",
    "\n",
    "        elif (word[0] in filtered_y) and not (word[1].startswith('NNP') or word[1].startswith('NNPS')) :  \n",
    "            newword = rresuly[iiindey]\n",
    "            word_list[index] = newword\n",
    "            iiindey += 1\n",
    "\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_proper_nouns(words):\n",
    "    tagged_words = pos_tag(words)\n",
    "    proper_nouns = []\n",
    "    filtered_y = []\n",
    "    proper_nouns_pos = []\n",
    "\n",
    "    for word, pos in tagged_words:\n",
    "        if pos.startswith('NNP') or pos.startswith('NNPS'):\n",
    "            proper_nouns.append(word)\n",
    "        else:\n",
    "            filtered_y.append(word)\n",
    "        proper_nouns_pos.append((word,pos))\n",
    "    return proper_nouns, proper_nouns_pos, filtered_y\n",
    "\n",
    "\n",
    "def zxy_split(word_list, proof_number):\n",
    "    proper_nouns, proper_nouns_pos, filtered_y = filter_proper_nouns(word_list)\n",
    "    filtered = list(set(word_list) - set(proper_nouns))\n",
    "\n",
    "    if  proof_number == 1:\n",
    "        new_word_list = proof1_change(word_list,  proper_nouns, filtered, proper_nouns_pos)\n",
    "\n",
    "    elif proof_number == 2:\n",
    "        new_word_list = proof2_change(word_list,  proper_nouns, filtered, proper_nouns_pos)\n",
    "\n",
    "    elif proof_number == 3:\n",
    "        new_word_list = proof3_change(word_list,  proper_nouns, filtered, proper_nouns_pos)\n",
    "\n",
    "    elif proof_number == 6:\n",
    "        proper_nouns_x = list(set(proper_nouns))\n",
    "        new_word_list = proof6_change(word_list,  proper_nouns, filtered, proper_nouns_pos, filtered_y, proper_nouns_x)\n",
    "    elif proof_number == 0:    \n",
    "        new_word_list = word_list\n",
    "    else:\n",
    "        pass\n",
    "    return new_word_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geshj_tra_tes(data):\n",
    "    for index in range(len(data)):\n",
    "        if ((index+1) % 1000 == 0):\n",
    "            print(index)\n",
    "        atexta = data.examples[index].text\n",
    "        filtered_aaa0 = [word for word in atexta if re.match(r'^[A-Za-z.,!?0-9]+$', word)]\n",
    "        word_listaaa = zxy_split(filtered_aaa0, proof_number)\n",
    "        data.examples[index].text = word_listaaa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chulia(astring):\n",
    "    astring = astring.strip('\\n').split('#')\n",
    "    astring = [i for i in astring if len(i)>0]\n",
    "    return astring\n",
    "\n",
    "def train_test_read(tr_te, data):\n",
    "    for index in range(len(tr_te)): \n",
    "        word_listaaa = data[index]\n",
    "        tr_te.examples[index].text = word_listaaa\n",
    "        if (file_args_key == \"live\") and (index % 1000 == 0):\n",
    "            print(index)\n",
    "        if (file_args_key == \"shake\") and (index % args.B == 0):\n",
    "            print(index)\n",
    "\n",
    "def read_or_write(key, proof_number):\n",
    "    # file_args_key = \"live\"\n",
    "    # file_args_key = \"shake\"\n",
    "    if proof_number == 1:\n",
    "        if file_args_key == \"live\":\n",
    "            file_name_train = 'train_data_change_result_proof1_norm.txt'\n",
    "            file_name_test  = 'test_data_change_result_proof1_norm.txt'\n",
    "        elif file_args_key == \"shake\":\n",
    "            file_name_train = 'train_proof1_50_norm.txt'\n",
    "            file_name_test = 'test__proof1_50_norm.txt'\n",
    "\n",
    "    if proof_number == 2:\n",
    "        if file_args_key == \"live\":\n",
    "            file_name_train = 'train_data_change_result_proof2_norm.txt'\n",
    "            file_name_test = 'test_data_change_result_proof2_norm.txt'\n",
    "        elif file_args_key == \"shake\":\n",
    "            file_name_train = 'train_proof2_50_norm.txt'\n",
    "            file_name_test = 'test__proof2_50_norm.txt'\n",
    "\n",
    "    if proof_number == 3:\n",
    "        if file_args_key == \"live\":\n",
    "            file_name_train = 'train_data_change_result_proof3_norm.txt'\n",
    "            file_name_test = 'test_data_change_result_proof3_norm.txt'\n",
    "        elif file_args_key == \"shake\":\n",
    "            file_name_train = 'train_proof3_50_norm.txt'\n",
    "            file_name_test = 'test__proof3_50_norm.txt'\n",
    "\n",
    "    if proof_number == 6:\n",
    "        if file_args_key == \"live\":\n",
    "            file_name_train = 'train_data_change_result_proof6_norm.txt'\n",
    "            file_name_test  = 'test_data_change_result_proof6_norm.txt'\n",
    "        elif file_args_key == \"shake\":\n",
    "            file_name_train = 'train_proof6_50_norm.txt'\n",
    "            file_name_test = 'test__proof6_50_norm.txt'\n",
    "    \n",
    "    if proof_number == 0:\n",
    "        if file_args_key == \"live\":\n",
    "            file_name_train = 'train_data_change_result_proof0_norm.txt'\n",
    "            file_name_test  = 'test_data_change_result_proof0_norm.txt'\n",
    "        elif file_args_key == \"shake\":\n",
    "            file_name_train = 'train_proof0_50_norm.txt'\n",
    "            file_name_test = 'test__proof0_50_norm.txt'\n",
    "\n",
    "    file_name_train = write_read_txt + \"/\" + file_name_train\n",
    "    file_name_test = write_read_txt + \"/\" + file_name_test\n",
    "\n",
    "    if key=='read':\n",
    "        fead_train = open(file_name_train, 'r', encoding='utf-8').readlines()\n",
    "        fead_test = open(file_name_test, 'r', encoding='utf-8').readlines()\n",
    "        train_test_read(train_data, fead_train)\n",
    "        train_test_read(test_data, fead_test)\n",
    "\n",
    "    elif key=='write':\n",
    "        geshj_tra_tes(train_data)\n",
    "        file_write(file_name_train, len_train_data, train_data)\n",
    "        geshj_tra_tes(test_data)\n",
    "        file_write(file_name_test, len_test_data, test_data)\n",
    "\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "def list_to_string(alist):\n",
    "    string = ''\n",
    "    for i in alist:\n",
    "        string += i + '#'\n",
    "    return string\n",
    "\n",
    "def file_write(filename, len_data, data):\n",
    "    f = open(filename, 'w', encoding = 'utf-8')\n",
    "    for index in range(len_data):\n",
    "        strings = list_to_string(data.examples[index].text)\n",
    "        f.write(strings + '\\n')\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999\n",
      "1999\n",
      "2999\n",
      "3999\n",
      "4999\n",
      "5999\n",
      "6999\n",
      "7999\n",
      "8999\n",
      "9999\n",
      "10999\n",
      "11999\n",
      "12999\n",
      "13999\n",
      "14999\n",
      "15999\n",
      "16999\n",
      "17999\n",
      "18999\n",
      "19999\n",
      "20999\n",
      "21999\n",
      "22999\n",
      "23999\n",
      "24999\n",
      "999\n",
      "1999\n",
      "2999\n",
      "3999\n",
      "4999\n",
      "5999\n",
      "6999\n",
      "7999\n",
      "8999\n",
      "9999\n",
      "10999\n",
      "11999\n",
      "12999\n",
      "13999\n",
      "14999\n",
      "15999\n",
      "16999\n",
      "17999\n",
      "18999\n",
      "19999\n",
      "20999\n",
      "21999\n",
      "22999\n",
      "23999\n",
      "24999\n"
     ]
    }
   ],
   "source": [
    "read_or_write(key_read_write, proof_number)\n",
    "if raise_Exception and (key_read_write == 'write') :\n",
    "    raise Exception(\"Terminated at this point\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = list(range(len(train_data.examples)))\n",
    "\n",
    "k = args.K \n",
    "random_seed = 42  \n",
    "kfold = KFold(n_splits=k, shuffle=True, random_state=random_seed)\n",
    "\n",
    "train_subsets = []\n",
    "for _, train_idx in kfold.split(train_indices):\n",
    "    train_subset = data.Dataset([train_data.examples[i] for i in train_idx], fields=[('text', TEXT), ('label', LABEL)])\n",
    "    train_subsets.append(train_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_indices = list(range(len(test_data.examples)))\n",
    "\n",
    "random_seed = 24  \n",
    "kfold_te = KFold(n_splits=k, shuffle=True, random_state=random_seed)\n",
    "\n",
    "test_subsets = []\n",
    "for _, test_idx in kfold_te.split(test_indices):\n",
    "    test_subset = data.Dataset([test_data.examples[i] for i in test_idx], fields=[('text', TEXT), ('label', LABEL)])\n",
    "    test_subsets.append(test_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterator(tri_data, tes_data):\n",
    "    \n",
    "    train_iterator, test_iterator = data.BucketIterator.splits(\n",
    "        (tri_data, tes_data),\n",
    "        batch_size = args.B,\n",
    "        sort_key = lambda x: len(x.text),\n",
    "        sort_within_batch=True,\n",
    "        device=torch.device(\"cpu\") \n",
    "    )\n",
    "\n",
    "    return train_iterator, test_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all_sub = []\n",
    "testa_all_sub = []\n",
    "\n",
    "for index in range(len(train_subsets)) :\n",
    "    data_i, data_j = train_subsets[index ], test_subsets[index ]\n",
    "    data_i_reu, data_j_reu = iterator(data_i, data_j)\n",
    "    train_all_sub.append(data_i_reu)\n",
    "    testa_all_sub.append(data_j_reu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.BCEWithLogitsLoss().to(args.device) \n",
    "loss_function_gradient = nn.BCEWithLogitsLoss().to(args.device) \n",
    "\n",
    "def get_val_loss(model, Vall):  \n",
    "    model = model.to(args.device)\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    with torch.no_grad():\n",
    "        for seqq in Vall:\n",
    "            text = seqq.text.to(args.device)\n",
    "            target = seqq.label.to(args.device)\n",
    "            y_preda = model(text).squeeze(1)\n",
    "            loss = loss_function(y_preda, target)\n",
    "            val_loss.append(loss.item())\n",
    "    model = model.to('cpu')        \n",
    "    loss_v = np.array(val_loss).mean()\n",
    "    return loss_v \n",
    "\n",
    "def binary_acc(preds, y):\n",
    "    preds = preds.to('cpu')\n",
    "    y = y.to('cpu')\n",
    "    preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = torch.eq(preds, y).float()\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "def test(args, model, k):\n",
    "    model.eval()\n",
    "    Val = testa_all_sub[k] \n",
    "    avg_acc = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for seq in Val:\n",
    "            text = seq.text.to(args.device)\n",
    "            y_pred = model(text).squeeze(1) \n",
    "            y_pred = y_pred.to('cpu')\n",
    "            target = seq.label.to('cpu')\n",
    "            acc = binary_acc(y_pred, target).item()\n",
    "            avg_acc.append(acc)\n",
    "    avg_acc = np.mean(avg_acc)\n",
    "    \n",
    "    print(\"客户端：\", k, ' avg acc', avg_acc)\n",
    "\n",
    "    wstring = f\"客户端：{k} avg acc: {avg_acc}\"\n",
    "    writecon = str(wstring) + \"\\n\"\n",
    "    file_write_log.write(writecon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectsampling(x):\n",
    "    custom_min = 0.0001\n",
    "    custom_max = 0.9999\n",
    "    scaled_data = (x.max() - x) / (x.max() - x.min()) * (custom_max - custom_min) + custom_min\n",
    "    probs = F.softmax(scaled_data, dim=0)\n",
    "    return probs    \n",
    "\n",
    "def random_selectsampling_batch(Len):\n",
    "    return torch.full([Len], 1.0 / Len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yangbenshaixuan_batch(Dtr, norms, sampling_rate, keynumber0816, Len):\n",
    "    if keynumber0816 == 1:\n",
    "        pi = selectsampling(norms)\n",
    "    elif keynumber0816 == 4:\n",
    "        pi = random_selectsampling_batch(Len)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    num_samples_num = int(sampling_rate * len(pi))\n",
    "    try:\n",
    "        samples = torch.multinomial(pi, num_samples= num_samples_num, replacement=False)\n",
    "    except:\n",
    "        print(\"---except---\")\n",
    "        pi = random_selectsampling_batch(Len)\n",
    "        samples = torch.multinomial(pi, num_samples= num_samples_num, replacement=False)\n",
    "\n",
    "    new_sample_batch = []\n",
    "    samples = samples.tolist()\n",
    "    iiindex_list = []\n",
    "    for iiindex, batch in enumerate(Dtr):\n",
    "        if iiindex in samples:\n",
    "            new_sample_batch.append(batch)\n",
    "            iiindex_list.append(iiindex)\n",
    "\n",
    "    wstring = f\"minigrid sampling batch:  {iiindex_list} \"\n",
    "    writecon = str(wstring) + \"\\n\"\n",
    "    file_write_log.write(writecon)\n",
    "    return new_sample_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_select_batch(Dtr, gradient_model, optimizer_gradient):\n",
    "    result = []\n",
    "    for ttime, x_data in enumerate(Dtr):\n",
    "        text = x_data.text.to(args.device)\n",
    "        target = x_data.label.to(args.device)\n",
    "        y_pred = gradient_model(text).squeeze(1)\n",
    "        loss = loss_function_gradient(y_pred, target)\n",
    "        loss.backward(retain_graph=True)\n",
    "        grad_vars = []\n",
    "        for param in gradient_model.parameters():\n",
    "            if param.grad is not None:\n",
    "                grad_vars.append(param.grad.cpu().detach().clone())\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        for param in gradient_model.parameters():\n",
    "            if param.grad is not None:\n",
    "                a_grad = param.grad.detach_()\n",
    "                param.grad.zero_()\n",
    "        c = torch.cat([param.view(-1) for param in grad_vars]).unsqueeze(0).to(\"cpu\")\n",
    "        result.append(c)  \n",
    "\n",
    "    result = torch.cat(result, dim=0)\n",
    "    myresult = result.to('cpu')\n",
    "    norms = torch.norm(myresult, dim=1)\n",
    "    return  norms  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(args, model, k, gradient_model):  \n",
    "    Dtr = train_all_sub[k]   \n",
    "    Val = testa_all_sub[k]\n",
    "    \n",
    "    count = 0\n",
    "    for _ in Dtr:\n",
    "        count += 1\n",
    "\n",
    "    if args.optimizer == 'adam': \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr,\n",
    "                                     weight_decay=args.weight_decay)  \n",
    "        optimizer_gradient = torch.optim.Adam(gradient_model.parameters(), lr=args.lr,\n",
    "                                     weight_decay=args.weight_decay)  \n",
    "    else:\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=args.lr,\n",
    "                                    momentum=0.9, weight_decay=args.weight_decay)\n",
    "        optimizer_gradient = torch.optim.SGD(gradient_model.parameters(), lr=args.lr,\n",
    "                                    momentum=0.9, weight_decay=args.weight_decay)\n",
    "    min_epochs = 1\n",
    "    best_model = None\n",
    "    min_val_loss = 10000.0\n",
    "\n",
    "    for epoch in range(args.E):  \n",
    "        train_loss = [] \n",
    "        if keynumber0816 == 1:\n",
    "            gradient_model.load_state_dict(model.state_dict())\n",
    "            gradient_model.train()\n",
    "            gradient_model = gradient_model.to(args.device)\n",
    "            norms = real_select_batch(Dtr, gradient_model, optimizer_gradient)\n",
    "            gradient_model = gradient_model.to('cpu')\n",
    "            batch_list = yangbenshaixuan_batch(Dtr, norms , args.C, keynumber0816, count) #GGG\n",
    "\n",
    "        elif keynumber0816 == 4:\n",
    "            batch_list = yangbenshaixuan_batch(Dtr, None, args.batch_sample_rate, keynumber0816, count)\n",
    "        \n",
    "        model = model.to(args.device)\n",
    "        \n",
    "        for _ in range(2):  \n",
    "            for  X_train in  batch_list: \n",
    "                text = X_train.text.to(args.device)  \n",
    "                target = X_train.label.to(args.device)\n",
    "                model.train()\n",
    "                y_pred = model(text).squeeze(1)\n",
    "                loss = loss_function(y_pred, target)\n",
    "                train_loss.append(loss.item())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "        model = model.to('cpu')    \n",
    "        gradient_model.load_state_dict(model.state_dict())\n",
    "        val_loss = get_val_loss(model, Val)\n",
    "        if (file_args_key == \"shake\") and ((epoch+1) % 1 == 0) :\n",
    "            wstring = f\"epoch {epoch} 训练 train_loss {np.array(train_loss).mean()} 验证 val_loss {val_loss}\"\n",
    "            writecon = str(wstring) + \"\\n\"\n",
    "            file_write_log.write(writecon)\n",
    "\n",
    "        if (file_args_key == \"live\") and ((epoch+1) % 1 == 0):\n",
    "            wstring = f\"epoch {epoch} 训练 train_loss {np.array(train_loss).mean()} 验证 val_loss {val_loss}\"\n",
    "            writecon = str(wstring) + \"\\n\"\n",
    "            file_write_log.write(writecon)\n",
    "\n",
    "        if (epoch + 1 >= min_epochs) and (val_loss <= min_val_loss) : \n",
    "            min_val_loss = val_loss\n",
    "        \n",
    "    model = model.to('cpu')\n",
    "    avg_val_lossa = np.array(train_loss).mean()\n",
    "    wstring = f\"model {k} val_loss: {avg_val_lossa} \"\n",
    "    writecon = str(wstring) + \"\\n\"\n",
    "    file_write_log.write(writecon)\n",
    "    wstring = f\"训练 best min train_loss {min_val_loss} \"\n",
    "    writecon = str(wstring) + \"\\n\"\n",
    "    file_write_log.write(writecon)\n",
    "    print('model {}, avg_loss {:.8f},  min_loss {:.8f}, val loss {:.8f} '.format(k, avg_val_lossa,min_val_loss, val_loss ))\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FedAvg:\n",
    "\n",
    "    def __init__(self, args):\n",
    "\n",
    "        self.args = args \n",
    "        self.clients = args.clients  \n",
    "        \n",
    "        if (proof_number == 1) or (proof_number == 3) or (proof_number == 6):\n",
    "            self.nn = RNN50(vocab_size, args, name='server').to('cpu')   # \n",
    "            self.nn.embedding.weight.data.copy_(new_pretrained_embedding)\n",
    "            \n",
    "            self.nn_gradient = RNN50_gradient(vocab_size, args, name='nn_gradient').to('cpu')  \n",
    "            self.nn_gradient.embedding.weight.data.copy_(new_pretrained_embedding)\n",
    "        \n",
    "        elif (proof_number == 0) or (proof_number == 2) :\n",
    "            self.nn = RNN(vocab_size, args, name='server').to('cpu')\n",
    "            self.nn.embedding.weight.data.copy_(pretrained_embedding)\n",
    "\n",
    "            self.nn_gradient = RNN_gradient(vocab_size, args, name='nn_gradient').to('cpu')\n",
    "            self.nn_gradient.embedding.weight.data.copy_(pretrained_embedding)\n",
    "\n",
    "        self.nns = []\n",
    "        self.m = 0\n",
    "        \n",
    "        for i in range(self.args.K):   \n",
    "            temp = copy.deepcopy(self.nn)\n",
    "            temp.name = self.clients[i]  \n",
    "            self.nns.append(temp)  \n",
    "\n",
    "    def server(self):\n",
    "        for t in range(self.args.r):  \n",
    "            print('round：--------', t + 1, '------------------------')\n",
    "            wstring = f\"round：-------- {t + 1} ------------------------\"\n",
    "            writecon = str(wstring) + \"\\n\"\n",
    "            file_write_log.write(writecon)\n",
    "            m = max(int(self.args.C * self.args.K), 1)\n",
    "            index = torch.randperm(self.args.K)[:m].tolist()\n",
    "            self.m = m\n",
    "            print('选取客户端:',index)\n",
    "\n",
    "            wstring = f\"选取客户端: {index} \"\n",
    "            writecon = str(wstring) + \"\\n\"\n",
    "            file_write_log.write(writecon)\n",
    "\n",
    "            self.dispatch(index) \n",
    "            self.client_update(index)  \n",
    "            self.aggregation(index)  \n",
    "\n",
    "        torch.save(self.nn.state_dict(), save_name_pth)\n",
    "        print(save_name_pth, 'save')\n",
    "        return self.nn\n",
    "\n",
    "    def dispatch(self, index): \n",
    "        self.nn = self.nn.to('cpu')\n",
    "        for j in index:\n",
    "            self.nns[j] = self.nns[j].to('cpu')\n",
    "            for old_params, new_params in zip(self.nns[j].parameters(), self.nn.parameters()):\n",
    "                old_params.data = new_params.data.clone()\n",
    "            self.nns[j] = self.nns[j].to('cpu')\n",
    "        self.nn = self.nn.to('cpu')\n",
    "\n",
    "    def client_update(self, index): \n",
    "        if batch_sanpling_bur:\n",
    "            for k in index: \n",
    "                wstring = f\"训练本地模型： {k} \"\n",
    "                writecon = str(wstring) + \"\\n\"\n",
    "                file_write_log.write(writecon)\n",
    "                self.nns[k] = self.nns[k].to('cpu')\n",
    "                self.nns[k] = train_batch(self.args, self.nns[k], k, self.nn_gradient)  \n",
    "\n",
    "\n",
    "    def aggregation(self, index):  \n",
    "        s = 0\n",
    "\n",
    "        params = {}\n",
    "        self.nns[0] = self.nns[0].to('cpu')\n",
    "        for k, v in self.nns[0].named_parameters(): \n",
    "            params[k] = torch.zeros_like(v.data)\n",
    "\n",
    "        for j in index:\n",
    "            self.nns[j] = self.nns[j].to('cpu')\n",
    "            for k, v in self.nns[j].named_parameters():  \n",
    "                params[k] += v.data * (1.0 / self.m ) \n",
    "\n",
    "        for k, v in self.nn.named_parameters():\n",
    "            v.data = params[k].data.clone() \n",
    "\n",
    "    def global_test(self, args): \n",
    "        model = self.nn.to(args.device) \n",
    "        c = args.clients \n",
    "        for k, client in enumerate(c):\n",
    "            test(self.args, model, k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round：-------- 1 ------------------------\n",
      "选取客户端: [16, 14, 7, 15, 17, 18, 12, 3, 6, 9]\n",
      "model 16, avg_loss 0.03561878,  min_loss 0.69264111, val loss 1.81281111 \n",
      "model 14, avg_loss 0.41568672,  min_loss 0.68919823, val loss 0.79486532 \n",
      "model 7, avg_loss 0.07939017,  min_loss 0.69314045, val loss 1.41536689 \n",
      "model 15, avg_loss 0.06293663,  min_loss 0.69053440, val loss 1.60556977 \n",
      "model 17, avg_loss 0.05497739,  min_loss 0.69287919, val loss 1.67179835 \n",
      "model 18, avg_loss 0.55379901,  min_loss 0.68827076, val loss 0.83264735 \n",
      "model 12, avg_loss 0.02339429,  min_loss 0.69179591, val loss 2.18660032 \n",
      "model 3, avg_loss 0.03766901,  min_loss 0.69287143, val loss 1.95140138 \n",
      "model 6, avg_loss 0.06569693,  min_loss 0.68461872, val loss 1.63676541 \n",
      "model 9, avg_loss 0.03744979,  min_loss 0.69397536, val loss 1.32547286 \n",
      "round：-------- 2 ------------------------\n",
      "选取客户端: [18, 7, 12, 15, 9, 11, 0, 2, 8, 10]\n",
      "model 18, avg_loss 0.68558748,  min_loss 0.68779528, val loss 0.68944121 \n",
      "model 7, avg_loss 0.08004206,  min_loss 0.69120803, val loss 1.43145824 \n",
      "model 12, avg_loss 0.06895487,  min_loss 0.68453784, val loss 1.64267204 \n",
      "model 15, avg_loss 0.38487450,  min_loss 0.68585765, val loss 0.90194487 \n",
      "model 9, avg_loss 0.02953009,  min_loss 0.67356373, val loss 1.66623297 \n",
      "model 11, avg_loss 0.04951740,  min_loss 0.67320203, val loss 1.50541638 \n",
      "model 0, avg_loss 0.05603854,  min_loss 0.66936299, val loss 1.53948987 \n",
      "model 2, avg_loss 0.10455236,  min_loss 0.68794569, val loss 1.19078890 \n",
      "model 8, avg_loss 0.02598102,  min_loss 0.69200924, val loss 1.92051452 \n",
      "model 10, avg_loss 0.03671558,  min_loss 0.67846497, val loss 1.57782012 \n",
      "round：-------- 3 ------------------------\n",
      "选取客户端: [19, 6, 0, 11, 16, 15, 7, 13, 5, 3]\n",
      "model 19, avg_loss 0.01999634,  min_loss 0.63308186, val loss 1.91416873 \n",
      "model 6, avg_loss 0.07160167,  min_loss 0.62889255, val loss 1.64014865 \n",
      "model 0, avg_loss 0.05190049,  min_loss 0.66121792, val loss 1.79162470 \n",
      "model 11, avg_loss 0.14206832,  min_loss 0.67317673, val loss 0.93803940 \n",
      "model 16, avg_loss 0.02269456,  min_loss 0.67880279, val loss 2.06438021 \n",
      "model 15, avg_loss 0.06572888,  min_loss 0.67980776, val loss 1.45050431 \n",
      "model 7, avg_loss 0.02334443,  min_loss 0.73257951, val loss 1.74060521 \n",
      "model 13, avg_loss 0.04003577,  min_loss 0.65660421, val loss 1.53596835 \n",
      "model 5, avg_loss 0.53344731,  min_loss 0.63438568, val loss 0.87420048 \n",
      "model 3, avg_loss 0.69376009,  min_loss 0.64738911, val loss 0.70192542 \n",
      "round：-------- 4 ------------------------\n",
      "选取客户端: [5, 8, 12, 4, 19, 10, 2, 6, 7, 1]\n",
      "model 5, avg_loss 0.01352904,  min_loss 0.61770805, val loss 1.70409466 \n",
      "model 8, avg_loss 0.00907285,  min_loss 0.71876695, val loss 2.26453239 \n",
      "model 12, avg_loss 0.01134139,  min_loss 0.65936447, val loss 1.94559742 \n",
      "model 4, avg_loss 0.01837973,  min_loss 0.65567574, val loss 2.18017913 \n",
      "model 19, avg_loss 0.02933011,  min_loss 0.64630590, val loss 2.09954892 \n",
      "model 10, avg_loss 0.02104623,  min_loss 0.70156126, val loss 2.24240427 \n",
      "model 2, avg_loss 0.02845569,  min_loss 0.68329598, val loss 1.27358422 \n",
      "model 6, avg_loss 0.04670092,  min_loss 0.63044354, val loss 1.45237377 \n",
      "model 7, avg_loss 0.17706204,  min_loss 0.63921270, val loss 1.31722428 \n",
      "model 1, avg_loss 0.03285130,  min_loss 0.63480652, val loss 1.68368538 \n",
      "round：-------- 5 ------------------------\n",
      "选取客户端: [4, 3, 11, 10, 15, 17, 19, 5, 12, 13]\n",
      "model 4, avg_loss 0.01082715,  min_loss 0.72635611, val loss 1.99958449 \n",
      "model 3, avg_loss 0.03115898,  min_loss 0.59859096, val loss 2.09838251 \n",
      "model 11, avg_loss 0.01109832,  min_loss 0.62069863, val loss 1.84506433 \n",
      "model 10, avg_loss 0.01634469,  min_loss 0.65660191, val loss 1.69649688 \n",
      "model 15, avg_loss 0.01555131,  min_loss 0.66096268, val loss 1.96529887 \n",
      "model 17, avg_loss 0.03738869,  min_loss 0.60038768, val loss 1.29752311 \n",
      "model 19, avg_loss 0.03147482,  min_loss 0.65026421, val loss 1.85056809 \n",
      "model 5, avg_loss 0.02377444,  min_loss 0.63572080, val loss 1.68319003 \n",
      "model 12, avg_loss 0.01832636,  min_loss 0.65808831, val loss 2.16855826 \n",
      "model 13, avg_loss 0.45832329,  min_loss 0.60428712, val loss 0.72768840 \n",
      "round：-------- 6 ------------------------\n",
      "选取客户端: [18, 19, 6, 11, 0, 14, 16, 10, 2, 17]\n",
      "model 18, avg_loss 0.03647454,  min_loss 0.61222022, val loss 1.65387625 \n",
      "model 19, avg_loss 0.01303168,  min_loss 0.60264540, val loss 2.14054722 \n",
      "model 6, avg_loss 0.02102676,  min_loss 0.70416712, val loss 2.29820677 \n",
      "model 11, avg_loss 0.02481464,  min_loss 0.68017036, val loss 1.39877022 \n",
      "model 0, avg_loss 0.04038714,  min_loss 0.65484734, val loss 1.25237940 \n",
      "model 14, avg_loss 0.01863390,  min_loss 0.67343150, val loss 2.17628210 \n",
      "model 16, avg_loss 0.03382320,  min_loss 0.70363070, val loss 2.01052218 \n",
      "model 10, avg_loss 0.03912350,  min_loss 0.68870714, val loss 1.08088140 \n",
      "model 2, avg_loss 0.01548820,  min_loss 0.68694339, val loss 1.70579499 \n",
      "model 17, avg_loss 0.10361679,  min_loss 0.63354843, val loss 2.20894140 \n",
      "round：-------- 7 ------------------------\n",
      "选取客户端: [18, 0, 5, 12, 11, 17, 8, 14, 16, 10]\n",
      "model 18, avg_loss 0.02420605,  min_loss 0.99501763, val loss 2.18360004 \n",
      "model 0, avg_loss 0.01310696,  min_loss 0.73133137, val loss 2.01934475 \n",
      "model 5, avg_loss 0.02175487,  min_loss 0.65545374, val loss 1.77441933 \n",
      "model 12, avg_loss 0.02040444,  min_loss 0.67307675, val loss 1.73811906 \n",
      "model 11, avg_loss 0.02466061,  min_loss 0.66029261, val loss 1.44458715 \n",
      "model 17, avg_loss 0.00796814,  min_loss 0.69177968, val loss 2.21414716 \n",
      "model 8, avg_loss 0.04889303,  min_loss 0.73234916, val loss 2.11075825 \n",
      "model 14, avg_loss 0.07264107,  min_loss 0.84017489, val loss 1.70627315 \n",
      "model 16, avg_loss 0.01115260,  min_loss 0.62446647, val loss 2.01033864 \n",
      "model 10, avg_loss 0.02420242,  min_loss 0.63676296, val loss 2.11752053 \n",
      "round：-------- 8 ------------------------\n",
      "选取客户端: [10, 13, 7, 3, 9, 15, 5, 1, 18, 2]\n",
      "model 10, avg_loss 0.00227342,  min_loss 0.68507532, val loss 2.32048719 \n",
      "model 13, avg_loss 0.02046038,  min_loss 0.64939805, val loss 1.52951830 \n",
      "model 7, avg_loss 0.00950549,  min_loss 0.65055512, val loss 2.11079232 \n",
      "model 3, avg_loss 0.03529131,  min_loss 0.69933465, val loss 1.16283596 \n",
      "model 9, avg_loss 0.05264131,  min_loss 0.71168416, val loss 1.60105197 \n",
      "model 15, avg_loss 0.03999858,  min_loss 0.72298197, val loss 1.70125657 \n",
      "model 5, avg_loss 0.01957113,  min_loss 0.66660182, val loss 1.80110940 \n",
      "model 1, avg_loss 0.03208551,  min_loss 0.73086713, val loss 1.53213430 \n",
      "model 18, avg_loss 0.00538714,  min_loss 0.74401516, val loss 2.05974843 \n",
      "model 2, avg_loss 0.01435465,  min_loss 0.67243373, val loss 2.08537139 \n",
      "round：-------- 9 ------------------------\n",
      "选取客户端: [12, 9, 8, 18, 16, 0, 7, 10, 6, 5]\n",
      "model 12, avg_loss 0.02174597,  min_loss 0.63853120, val loss 1.73694372 \n",
      "model 9, avg_loss 0.01561803,  min_loss 0.60025149, val loss 1.83564269 \n",
      "model 8, avg_loss 0.00628995,  min_loss 0.70720699, val loss 2.11586164 \n",
      "model 18, avg_loss 0.00795846,  min_loss 0.95784374, val loss 2.15717848 \n",
      "model 16, avg_loss 0.00659936,  min_loss 0.67192444, val loss 2.15774158 \n",
      "model 0, avg_loss 0.01264562,  min_loss 0.68226692, val loss 1.67128367 \n",
      "model 7, avg_loss 0.01813923,  min_loss 0.74003310, val loss 1.78950003 \n",
      "model 10, avg_loss 0.01343616,  min_loss 0.80904232, val loss 1.77377782 \n",
      "model 6, avg_loss 0.03201690,  min_loss 0.64536061, val loss 1.93170172 \n",
      "model 5, avg_loss 0.02295719,  min_loss 0.72746948, val loss 1.86100417 \n",
      "round：-------- 10 ------------------------\n",
      "选取客户端: [16, 18, 9, 2, 3, 17, 11, 14, 15, 7]\n",
      "model 16, avg_loss 0.03248340,  min_loss 0.78678052, val loss 2.27507617 \n",
      "model 18, avg_loss 0.73754012,  min_loss 0.71180293, val loss 0.71180293 \n",
      "model 9, avg_loss 0.00522685,  min_loss 0.60956889, val loss 2.02195930 \n",
      "model 2, avg_loss 0.01143572,  min_loss 0.64182253, val loss 1.59363849 \n",
      "model 3, avg_loss 0.01678459,  min_loss 0.70896363, val loss 2.10914791 \n",
      "model 17, avg_loss 0.00762680,  min_loss 0.67144839, val loss 1.63602825 \n",
      "model 11, avg_loss 0.03510057,  min_loss 0.71047868, val loss 1.72376660 \n",
      "model 14, avg_loss 0.03041405,  min_loss 0.60483232, val loss 1.64598254 \n",
      "model 15, avg_loss 0.02187581,  min_loss 0.65916600, val loss 1.61012250 \n",
      "model 7, avg_loss 0.01901359,  min_loss 0.87110666, val loss 1.93966355 \n",
      "/mnt/workspace/fedavg_ceshi/out_pth/live_6_4_0_30_0.5_write_08_27_2023_11_08_42_dict.pth save\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RNN50(\n",
       "  (embedding): Embedding(10002, 50)\n",
       "  (rnn): LSTM(50, 256, num_layers=2, dropout=0.5, bidirectional=True)\n",
       "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fed = FedAvg(args)\n",
    "fed.server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "客户端： 0  avg acc 0.7043650817303431\n",
      "客户端： 1  avg acc 0.678968262104761\n",
      "客户端： 2  avg acc 0.6777777806633994\n",
      "客户端： 3  avg acc 0.6932539734102431\n",
      "客户端： 4  avg acc 0.7103174641018822\n",
      "客户端： 5  avg acc 0.6944444463366554\n",
      "客户端： 6  avg acc 0.7222222245874859\n",
      "客户端： 7  avg acc 0.6904761954432442\n",
      "客户端： 8  avg acc 0.7023809552192688\n",
      "客户端： 9  avg acc 0.7134920642489478\n",
      "客户端： 10  avg acc 0.6960317464101882\n",
      "客户端： 11  avg acc 0.6821428608326685\n",
      "客户端： 12  avg acc 0.6888888875643412\n",
      "客户端： 13  avg acc 0.6956349235205423\n",
      "客户端： 14  avg acc 0.6849206373805091\n",
      "客户端： 15  avg acc 0.7000000029802322\n",
      "客户端： 16  avg acc 0.6976190535795121\n",
      "客户端： 17  avg acc 0.6936507948807308\n",
      "客户端： 18  avg acc 0.7079365111532665\n",
      "客户端： 19  avg acc 0.6980158715021043\n"
     ]
    }
   ],
   "source": [
    "fed.global_test(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_write_log.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e3f128cad0f25ad952b91777a639b4fa2222dfd0378c44e8bdbd611468a92202"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
