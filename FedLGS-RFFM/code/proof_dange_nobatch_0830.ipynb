{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_args_key = \"shake\"  \n",
    "# file_args_key = \"live\"\n",
    "\n",
    "proof_number = 0 # 0, 1,2, 3, 6\n",
    "keynumber0816 = 81 # 1、4、80、81\n",
    "\n",
    "# yesorno_inputdim = 100\n",
    "yesorno_inputdim = 50  \n",
    "\n",
    "kkkkkkkkkkkk = 0   # 0、1、2、3、4\n",
    "\n",
    "key_read_write = 'write'  \n",
    "# key_read_write = 'read'  \n",
    "\n",
    "# raise_Exception = True  \n",
    "raise_Exception = False \n",
    "\n",
    "batch_sanpling_bur = True \n",
    "# batch_sanpling_bur = False \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "没有小批次的实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext.legacy import data, datasets\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "import copy\n",
    "# from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "from model_nn import RNN, RNN50\n",
    "# from model_nn_min import RNN, RNN_gradient,RNN50, RNN50_gradient\n",
    "from args_fedavg import args_parser_live, args_parser_shake\n",
    "\n",
    "from nltk.tag import pos_tag\n",
    "import re\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_log = r\"/mnt/workspace/fedavg_ceshi/out_log\"\n",
    "out_pth = r\"/mnt/workspace/fedavg_ceshi/out_pth\"\n",
    "write_read_txt = r\"/mnt/workspace/fedavg_ceshi/write_read_txt\"\n",
    "\n",
    "if file_args_key == \"shake\":\n",
    "    args = args_parser_shake()\n",
    "elif file_args_key == \"live\":\n",
    "    args = args_parser_live()\n",
    "\n",
    "# batchsz = args.B\n",
    "# sampling_rate = args.C\n",
    "# cpu or GPU\n",
    "# device = args.device\n",
    "# clients_wind = args.clients\n",
    "\n",
    "String = time.strftime(\"%m_%d_%Y_%H_%M_%S\",time.localtime())\n",
    "namestr = str(String)\n",
    "\n",
    "file_write_log_name = out_log + \"/\" + \"log_\" + file_args_key + '_'  + str(proof_number) \\\n",
    "    + '_'+ str(keynumber0816) + '_'+ str(kkkkkkkkkkkk) + '_' \\\n",
    "        + str(args.B) + '_'+ str(args.C) + '_' + key_read_write + '_'+ namestr + \"_log.txt\"\n",
    "print(file_write_log_name)\n",
    "\n",
    "file_write_log = open(file_write_log_name, \"w\", encoding='utf-8')\n",
    "\n",
    "save_name_pth = out_pth + \"/\" +  file_args_key + '_'  + str(proof_number) \\\n",
    "    + '_'+ str(keynumber0816) + '_'+ str(kkkkkkkkkkkk) + '_' \\\n",
    "        + str(args.B) + '_'+ str(args.C) + '_' + key_read_write + '_'+ namestr +'_dict.pth'\n",
    "print(save_name_pth)\n",
    "\n",
    "writecon = str(save_name_pth) + \"\\n\"\n",
    "file_write_log.write(writecon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(tokenize='spacy')\n",
    "LABEL = data.LabelField(dtype=torch.float)\n",
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
    "\n",
    "if file_args_key == \"shake\":\n",
    "    # train_data.examples = train_data.examples[:len(train_data.examples)//500]\n",
    "    # test_data.examples = test_data.examples[:len(test_data.examples)//500]\n",
    "\n",
    "    train_data.examples = train_data.examples[:len(train_data.examples)//125]\n",
    "    test_data.examples = test_data.examples[:len(test_data.examples)//125]\n",
    "\n",
    "len_train_data = len(train_data)\n",
    "len_test_data = len(test_data)\n",
    "\n",
    "print('len of train data:', len(train_data))\n",
    "print('len of test data:', len(test_data))\n",
    "print(test_data.examples[1].text) \n",
    "print(test_data.examples[1].label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec, glove\n",
    "if yesorno_inputdim == 100:\n",
    "    TEXT.build_vocab(train_data, max_size=10000, vectors='glove.6B.100d')\n",
    "if yesorno_inputdim == 50:\n",
    "    TEXT.build_vocab(train_data, max_size=10000, vectors='glove.6B.50d')\n",
    "\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "# TEXT.build_vocab(train_data, max_size=10000, vectors='glove.6B.50d')\n",
    "# LABEL.build_vocab(train_data)\n",
    "\n",
    "# TEXT.build_vocab(test_data, max_size=10000, vectors='glove.6B.100d')\n",
    "# LABEL.build_vocab(test_data)\n",
    "\n",
    "vocab_size =  len(TEXT.vocab)\n",
    "pretrained_embedding = TEXT.vocab.vectors\n",
    "print(vocab_size, pretrained_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proof1、proof3、proof6 \n",
    "def RFFM(i, x):\n",
    "    torch.manual_seed(i)\n",
    "    input_dim = 100\n",
    "    output_dim = 50\n",
    "    sigma = 1.0\n",
    "    omega = torch.randn(output_dim, input_dim) * sigma\n",
    "    b = torch.rand(output_dim) * 2 * np.pi\n",
    "    x = x.view(-1, input_dim)\n",
    "    z = torch.cos(torch.mm(x, omega.t()) + b)\n",
    "    Z_Out =  torch.sqrt(2.0 / torch.tensor(output_dim)) * z\n",
    "    return Z_Out\n",
    "    # return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proof1、proof3、proof6 \n",
    "if yesorno_inputdim == 100:\n",
    "    new_vectors = []\n",
    "    for i, word in enumerate(TEXT.vocab.itos):\n",
    "        old_vector = pretrained_embedding[i]\n",
    "        new_vector = RFFM(i, old_vector)\n",
    "        new_vectors.append(new_vector)\n",
    "    new_pretrained_embedding = torch.stack(new_vectors, dim=0)\n",
    "    new_pretrained_embedding = new_pretrained_embedding.reshape(-1,50)\n",
    "    print(new_pretrained_embedding.shape)\n",
    "elif yesorno_inputdim == 50:\n",
    "    pass\n",
    "\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proof1、proof3、proof6 \n",
    "def word_vector_50(word):\n",
    "    # word = 'in'\n",
    "    word_index = TEXT.vocab.stoi[word]\n",
    "    # print(word_index)\n",
    "    word_vector = new_pretrained_embedding[word_index]\n",
    "    # print(word_vector)\n",
    "    return word_vector\n",
    "\n",
    "def proof1(a, b, epcluo):\n",
    "    a = a.to(torch.float)\n",
    "    b = b.to(torch.float)\n",
    "    dot_product = torch.dot(a, b)\n",
    "    norm_a = torch.norm(a, p=2)\n",
    "    norm_b = torch.norm(b, p=2)\n",
    "    similarity = dot_product / (norm_a * norm_b)\n",
    "    return torch.exp(similarity* epcluo / 2.0 *(-1))\n",
    "\n",
    "\n",
    "def proof1_change(word_list,  proper_nouns, filtered, proper_nouns_pos):\n",
    "    if len(proper_nouns) == 0 or len(filtered) == 0 :  #proper_nouns, filtered 任一为空\n",
    "        return word_list\n",
    "    # epcluo = args.epcluo\n",
    "    epcluo = args.epcluo_05\n",
    "    gam = []\n",
    "    xlist = [word_vector_50(word) for word in proper_nouns]\n",
    "    ylist = [word_vector_50(word) for word in filtered]\n",
    "    \n",
    "    for x in xlist:\n",
    "        x_list = []\n",
    "        for y in ylist:\n",
    "            res = proof1(x, y, epcluo)\n",
    "            x_list.append(res.item())\n",
    "        gam.append(x_list)\n",
    "    gam_tensor = torch.tensor(gam)\n",
    "    # print(gam_tensor)\n",
    "    row_sums = torch.sum(gam_tensor, dim=1, keepdim=True)  \n",
    "    result_prob = torch.div(gam_tensor, row_sums) \n",
    "\n",
    "    # probabilities = result_prob.clone()\n",
    "    # probabilities = result_prob\n",
    "    sample = torch.multinomial(result_prob, 1)\n",
    "    # print(sample)\n",
    "    # tensor = sample.clone()\n",
    "    # tensor = sample\n",
    "    list_result = sample.squeeze().tolist()\n",
    "    # print(list_result)\n",
    "    # print(filtered)\n",
    "    try:\n",
    "        list_result = list(list_result)\n",
    "    except:\n",
    "        list_result = [list_result]\n",
    "\n",
    "    rresul = [filtered[i] for i in list_result]\n",
    "    # print('-------------')\n",
    "    # print('rresul',longg, rresul)\n",
    "    iiindex = 0\n",
    "    for index, word in enumerate(proper_nouns_pos):\n",
    "        # print('index', index,  word)\n",
    "        if (word[0] in proper_nouns) and (word[1].startswith('NNP') or word[1].startswith('NNPS') ):\n",
    "            # print(iiindex)\n",
    "            newword = rresul[iiindex] \n",
    "            word_list[index] = newword\n",
    "            iiindex += 1   \n",
    "    return word_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_vector_100(word):\n",
    "    # word = 'in'\n",
    "    word_index = TEXT.vocab.stoi[word]\n",
    "    # print(word_index)\n",
    "    word_vector = TEXT.vocab.vectors[word_index]\n",
    "    # print(word_vector)\n",
    "    return word_vector\n",
    "\n",
    "def proof2(x, y, epcluo):\n",
    "    K = torch.exp(torch.norm(x-y, p=2).pow(2) / 2.0 *(-1) )\n",
    "    result = K * epcluo / 2.0 *(-1) \n",
    "    # print('result: ', result)\n",
    "    e14 = torch.exp(result)\n",
    "    # print(e14)\n",
    "    return e14\n",
    "\n",
    "\n",
    "def proof2_change(word_list,  proper_nouns, filtered, proper_nouns_pos):\n",
    "    if len(proper_nouns) == 0 or len(filtered) == 0 :  #proper_nouns, filtered 任一为空\n",
    "        return word_list\n",
    "    # epcluo = args.epcluo\n",
    "    epcluo = args.epcluo_05\n",
    "    gam = []\n",
    "    xlist = [word_vector_100(word) for word in proper_nouns]\n",
    "    ylist = [word_vector_100(word) for word in filtered]\n",
    "    \n",
    "    for x in xlist:\n",
    "        x_list = []\n",
    "        for y in ylist:\n",
    "            res = proof2(x, y, epcluo)\n",
    "            x_list.append(res.item())\n",
    "        gam.append(x_list)\n",
    "    gam_tensor = torch.tensor(gam)\n",
    "    # print(gam_tensor)\n",
    "    row_sums = torch.sum(gam_tensor, dim=1, keepdim=True)  \n",
    "    result_prob = torch.div(gam_tensor, row_sums)  \n",
    "    # probabilities = result_prob\n",
    "    sample = torch.multinomial(result_prob, 1)\n",
    "    # print(sample)\n",
    "    # tensor = sample\n",
    "    list_result = sample.squeeze().tolist()\n",
    "\n",
    "    # print(list_result)\n",
    "    # print(filtered)\n",
    "\n",
    "    try:\n",
    "        list_result = list(list_result)\n",
    "    except:\n",
    "        list_result = [list_result]\n",
    "    rresul = [filtered[i] for i in list_result]\n",
    "    # longg = len(rresul)\n",
    "    # print('-------------')\n",
    "    # print('rresul',longg, rresul)\n",
    "    iiindex = 0\n",
    "    for index, word in enumerate(proper_nouns_pos):\n",
    "        # print('index', index,  word)\n",
    "        # if (word in proper_nouns) and (iiindex < longg):\n",
    "        if (word[0] in proper_nouns) and (word[1].startswith('NNP') or word[1].startswith('NNPS') ):\n",
    "            # print(iiindex)\n",
    "            newword = rresul[iiindex]\n",
    "            word_list[index] = newword\n",
    "            iiindex += 1\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proof3(zx, zy, epcluo): \n",
    "    result = (torch.dot(zx, zy)) * epcluo / 4.0 *(-1) \n",
    "    # print('result: ', result)\n",
    "    e14 = torch.exp(result)\n",
    "    # print(e14)\n",
    "    return e14\n",
    "\n",
    "def proof3_change(word_list,  proper_nouns, filtered, proper_nouns_pos):\n",
    "    if len(proper_nouns) == 0 or len(filtered) == 0 :  \n",
    "        return word_list\n",
    "    # epcluo = args.epcluo\n",
    "    epcluo = args.epcluo_05\n",
    "    gam = []\n",
    "    xlist = [word_vector_50(word) for word in proper_nouns]\n",
    "    ylist = [word_vector_50(word) for word in filtered]\n",
    "\n",
    "    for x in xlist:\n",
    "        x_list = []\n",
    "        for y in ylist:\n",
    "            res = proof3(x, y, epcluo)\n",
    "            x_list.append(res.item())\n",
    "        gam.append(x_list)\n",
    "    gam_tensor = torch.tensor(gam)\n",
    "    # print(gam_tensor)\n",
    "    row_sums = torch.sum(gam_tensor, dim=1, keepdim=True)  \n",
    "    result_prob = torch.div(gam_tensor, row_sums)  \n",
    "    # print(result_prob)\n",
    "    # probabilities = result_prob\n",
    "    sample = torch.multinomial(result_prob, 1)\n",
    "    # print(sample)\n",
    "    # tensor = sample\n",
    "    list_result = sample.squeeze().tolist()\n",
    "\n",
    "    # print(list_result)\n",
    "    # print(filtered)\n",
    "    \n",
    "    try:\n",
    "        list_result = list(list_result)\n",
    "    except:\n",
    "        list_result = [list_result]\n",
    "    rresul = [filtered[i] for i in list_result]\n",
    "    # longg = len(rresul)\n",
    "    # print('-------------')\n",
    "    # print('rresul',longg, rresul)\n",
    "    iiindex = 0\n",
    "    # for index, word in enumerate(word_list):\n",
    "    for index, word in enumerate(proper_nouns_pos):\n",
    "        # print('index', index,  word)\n",
    "        # if (word in proper_nouns) and (iiindex < longg):\n",
    "        if (word[0] in proper_nouns) and (word[1].startswith('NNP') or word[1].startswith('NNPS') ):\n",
    "            # print(iiindex)\n",
    "            newword = rresul[iiindex]\n",
    "            word_list[index] = newword\n",
    "            iiindex += 1\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proof6(zx, zy, epcluo):\n",
    "\n",
    "    result = (torch.dot(zx, zy)) * epcluo / 4.0 *(-1) \n",
    "    # print('result: ', result)\n",
    "    e14 = torch.exp(result)\n",
    "    # print(e14)\n",
    "    return e14\n",
    "\n",
    "# def proof6_change(word_list,  proper_nouns, filtered, proper_nouns_pos):\n",
    "def proof6_change(word_list,  proper_nouns, filtered, proper_nouns_pos, filtered_y, proper_nouns_x ):\n",
    "    if len(proper_nouns) == 0 or len(filtered) == 0 :  \n",
    "        return word_list\n",
    "    # epcluo = args.epcluo\n",
    "    epcluo = args.epcluo_05\n",
    "    gam = []\n",
    "    xlist = [word_vector_50(word) for word in proper_nouns]\n",
    "    ylist = [word_vector_50(word) for word in filtered]\n",
    "    ylist_f = [word_vector_50(word) for word in filtered_y]\n",
    "    xlist_f = [word_vector_50(word) for word in proper_nouns_x]\n",
    "\n",
    "    for x in xlist:\n",
    "        x_list = []\n",
    "        for y in ylist:\n",
    "            res = proof6(x, y, epcluo)\n",
    "            x_list.append(res.item())\n",
    "        gam.append(x_list)\n",
    "\n",
    "    gam_tensor = torch.tensor(gam)\n",
    "    # print(gam_tensor)\n",
    "    row_sums = torch.sum(gam_tensor, dim=1, keepdim=True)  \n",
    "    result_prob = torch.div(gam_tensor, row_sums)  \n",
    "    # print(result_prob)\n",
    "    probabilities = result_prob.clone()\n",
    "    sample = torch.multinomial(probabilities, 1)\n",
    "    # print(sample)\n",
    "    tensor = sample.clone()\n",
    "    list_result = tensor.squeeze().tolist()\n",
    "    # print(list_result)\n",
    "    # print(filtered)\n",
    "    try:\n",
    "        list_result = list(list_result)\n",
    "    except:\n",
    "        list_result = [list_result]\n",
    "\n",
    "    rresul = [filtered[i] for i in list_result]\n",
    "    # longg = len(rresul)\n",
    "    # print('-------------')\n",
    "    # print('rresul',longg, rresul)\n",
    "    pr = args.pr\n",
    "    gam_pr = []\n",
    "    for y in ylist_f:\n",
    "        x_list_new = []\n",
    "        for x in xlist_f:\n",
    "            res = proof6(y, x, epcluo)\n",
    "            x_list_new.append(res.item())\n",
    "        gam_pr.append(x_list_new)\n",
    "\n",
    "    gam_pr_tensor = torch.tensor(gam_pr)\n",
    "    row_pr_sums = torch.sum(gam_pr_tensor, dim=1, keepdim=True)  \n",
    "    result_prob = pr * torch.div(gam_pr_tensor, row_pr_sums)  \n",
    "    column_to_add = torch.full((result_prob.shape[0], 1), pr)\n",
    "    tensor_with_column = torch.cat((result_prob, column_to_add), dim=1)\n",
    "    # sampling\n",
    "    probabilities = tensor_with_column.clone()\n",
    "    sample_pr = torch.multinomial(probabilities, 1)\n",
    "    # print(sample)\n",
    "    tensor_pr = sample_pr.clone()\n",
    "    list_result_pr = tensor_pr.squeeze().tolist()\n",
    "\n",
    "    try:\n",
    "        list_result_pr = list(list_result_pr)\n",
    "    except:\n",
    "        list_result_pr = [list_result_pr]\n",
    "\n",
    "    rresuly = []\n",
    "    xylong = len(tensor_with_column[0,:]) - 1\n",
    "\n",
    "    for index, yword in enumerate(list_result_pr):\n",
    "        if yword == xylong :\n",
    "            rresuly.append(filtered_y[index])\n",
    "        else:\n",
    "            rresuly.append(proper_nouns_x[yword])\n",
    "    # longgy = len(rresuly)\n",
    "    # print('-------------')\n",
    "    # print('rresul',longg, rresul)\n",
    "    iiindex = 0\n",
    "    iiindey = 0\n",
    "    for index, word in enumerate(proper_nouns_pos):\n",
    "        # print('index', index,  word)\n",
    "        # if (word in proper_nouns) and (iiindex < longg):\n",
    "        if (word[0] in proper_nouns) and (word[1].startswith('NNP') or word[1].startswith('NNPS') ):\n",
    "            # print(iiindex)\n",
    "            newword = rresul[iiindex]\n",
    "            word_list[index] = newword\n",
    "            iiindex += 1\n",
    "\n",
    "        elif (word[0] in filtered_y) and not (word[1].startswith('NNP') or word[1].startswith('NNPS')) :  \n",
    "        # else:\n",
    "            # print(iiindey)\n",
    "            newword = rresuly[iiindey]\n",
    "            word_list[index] = newword\n",
    "            iiindey += 1  \n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_proper_nouns(words):\n",
    "    tagged_words = pos_tag(words)\n",
    "    proper_nouns = []\n",
    "    filtered_y = []\n",
    "    proper_nouns_pos = []\n",
    "    for word, pos in tagged_words:\n",
    "        if pos.startswith('NNP') or pos.startswith('NNPS'):\n",
    "            proper_nouns.append(word)\n",
    "        else:\n",
    "            filtered_y.append(word)\n",
    "        proper_nouns_pos.append((word,pos))\n",
    "    return proper_nouns, proper_nouns_pos, filtered_y\n",
    "\n",
    "\n",
    "def zxy_split(word_list, proof_number):\n",
    "    proper_nouns, proper_nouns_pos, filtered_y = filter_proper_nouns(word_list)\n",
    "    # print(len(proper_nouns), proper_nouns)\n",
    "    filtered = list(set(word_list) - set(proper_nouns))\n",
    "    # print(filtered)\n",
    "    if  proof_number == 1:\n",
    "        new_word_list = proof1_change(word_list,  proper_nouns, filtered, proper_nouns_pos)\n",
    "\n",
    "    elif proof_number == 2:\n",
    "        new_word_list = proof2_change(word_list,  proper_nouns, filtered, proper_nouns_pos)\n",
    "\n",
    "    elif proof_number == 3:\n",
    "        new_word_list = proof3_change(word_list,  proper_nouns, filtered, proper_nouns_pos)\n",
    "\n",
    "    elif proof_number == 6:\n",
    "        proper_nouns_x = list(set(proper_nouns))\n",
    "        # proof6_change(word_list,  proper_nouns, filtered, proper_nouns_pos)\n",
    "        new_word_list = proof6_change(word_list,  proper_nouns, filtered, proper_nouns_pos, filtered_y, proper_nouns_x)\n",
    "    elif proof_number == 0:    \n",
    "        new_word_list = word_list\n",
    "    else:\n",
    "        pass\n",
    "    # print(new_word_list)\n",
    "    return new_word_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geshj_tra_tes(data, train_or_testa):\n",
    "    for index in range(len(data)):\n",
    "        if ((index+1) % 1000 == 0):\n",
    "            print(index)\n",
    "        # print(index)\n",
    "        # if index > 10:\n",
    "        #     break\n",
    "\n",
    "        # print('------------------------------')\n",
    "        # print(test_data.examples[index].text, test_data.examples[index].label)\n",
    "        atexta = data.examples[index].text\n",
    "        filtered_aaa0 = [word for word in atexta if re.match(r'^[A-Za-z.,!?0-9]+$', word)]\n",
    "        # print('---------------------:', filtered_aaa0)\n",
    "        if train_or_testa == \"oftrain\":\n",
    "            word_listaaa = zxy_split(filtered_aaa0, proof_number)\n",
    "            data.examples[index].text = word_listaaa\n",
    "        elif train_or_testa == \"oftesta\":\n",
    "            data.examples[index].text = filtered_aaa0\n",
    "        \n",
    "        # print('>>>>>>')\n",
    "        # print(test_data.examples[index].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chulia(astring):\n",
    "    astring = astring.strip('\\n').split('#')\n",
    "    astring = [i for i in astring if len(i)>0]\n",
    "    return astring\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "def train_test_read(tr_te, data):\n",
    "    for index in range(len(tr_te)):       \n",
    "        word_listaaa = data[index]\n",
    "        tr_te.examples[index].text = word_listaaa\n",
    "\n",
    "        # if index % 1000 == 0:\n",
    "        #     print(index)\n",
    "        if (file_args_key == \"live\") and (index % 1000 == 0):\n",
    "            print(index)\n",
    "        if (file_args_key == \"shake\") and (index % args.B == 0):\n",
    "            print(index)\n",
    "        # print('>>>>>>')\n",
    "        # print(test_data.examples[index].text)\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "def read_or_write(key, proof_number):\n",
    "    \n",
    "    # file_args_key = \"live\"\n",
    "    # file_args_key = \"shake\"\n",
    "    if proof_number == 1:\n",
    "        if file_args_key == \"live\":\n",
    "            file_name_train = 'train_data_change_result_proof1_Zout_sig.txt'\n",
    "            file_name_test  = 'test_data_change_result_proof1_Zout_sig.txt'\n",
    "        elif file_args_key == \"shake\":\n",
    "            file_name_train = 'train_proof1_50_Zout_sig.txt'\n",
    "            file_name_test = 'test__proof1_50_Zout_sig.txt'\n",
    "\n",
    "    if proof_number == 2:\n",
    "        if file_args_key == \"live\":\n",
    "            file_name_train = 'train_data_change_result_proof2_Zout_sig.txt'\n",
    "            file_name_test = 'test_data_change_result_proof2_Zout_sig.txt'\n",
    "        elif file_args_key == \"shake\":\n",
    "            file_name_train = 'train_proof2_50_Zout_sig.txt'\n",
    "            file_name_test = 'test__proof2_50_Zout_sig.txt'\n",
    "\n",
    "    if proof_number == 3:\n",
    "        if file_args_key == \"live\":\n",
    "            file_name_train = 'train_data_change_result_proof3_Zout_sig.txt'\n",
    "            file_name_test = 'test_data_change_result_proof3_Zout_sig.txt'\n",
    "        elif file_args_key == \"shake\":\n",
    "            file_name_train = 'train_proof3_50_Zout_sig.txt'\n",
    "            file_name_test = 'test__proof3_50_Zout_sig.txt'\n",
    "\n",
    "    if proof_number == 6:\n",
    "        if file_args_key == \"live\":\n",
    "            file_name_train = 'train_data_change_result_proof6_Zout_sig.txt'\n",
    "            file_name_test  = 'test_data_change_result_proof6_Zout_sig.txt'\n",
    "        elif file_args_key == \"shake\":\n",
    "            file_name_train = 'train_proof6_50_Zout_sig.txt'\n",
    "            file_name_test = 'test__proof6_50_Zout_sig.txt'\n",
    "    \n",
    "    if proof_number == 0:\n",
    "        if file_args_key == \"live\":\n",
    "            file_name_train = 'train_data_change_result_proof0_Zout_sig.txt'\n",
    "            file_name_test  = 'test_data_change_result_proof0_Zout_sig.txt'\n",
    "        elif file_args_key == \"shake\":\n",
    "            file_name_train = 'train_proof0_50_Zout_sig.txt'\n",
    "            file_name_test = 'test__proof0_50_Zout_sig.txt'\n",
    "\n",
    "    file_name_train = write_read_txt + \"/\" + file_name_train\n",
    "    file_name_test = write_read_txt + \"/\" + file_name_test\n",
    "\n",
    "    if key=='read':\n",
    "        fead_train = open(file_name_train, 'r', encoding='utf-8').readlines()\n",
    "        # fead_train = fead_train[:len(fead_train)//2]\n",
    "        fead_test = open(file_name_test, 'r', encoding='utf-8').readlines()\n",
    "        # fead_test = fead_test[:len(fead_test)//2]\n",
    "        # print(fead_test[:10])\n",
    "        # train_read(fead_train)\n",
    "        # test_read(fead_test)\n",
    "\n",
    "        train_test_read(train_data, fead_train)\n",
    "        train_test_read(test_data, fead_test)\n",
    "\n",
    "    elif key=='write':      \n",
    "        geshj_tra_tes(train_data, \"oftrain\")\n",
    "        file_write(file_name_train, len_train_data, train_data)\n",
    "\n",
    "        geshj_tra_tes(test_data, \"oftesta\")\n",
    "        file_write(file_name_test, len_test_data, test_data)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "def list_to_string(alist):\n",
    "    string = ''\n",
    "    for i in alist:\n",
    "        string += i + '#'\n",
    "    return string\n",
    "\n",
    "def file_write(filename, len_data, data):\n",
    "    f = open(filename, 'w', encoding = 'utf-8')\n",
    "    for index in range(len_data):\n",
    "        strings = list_to_string(data.examples[index].text)\n",
    "        f.write(strings + '\\n')\n",
    "    f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_or_write(key_read_write, proof_number)\n",
    "\n",
    "if raise_Exception and (key_read_write == 'write') :\n",
    "    raise Exception(\"Terminated at this point\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = list(range(len(train_data.examples)))\n",
    "\n",
    "k = args.K \n",
    "random_seed = 42  \n",
    "kfold = KFold(n_splits=k, shuffle=True, random_state=random_seed)\n",
    "\n",
    "train_subsets = []\n",
    "for _, train_idx in kfold.split(train_indices):\n",
    "    train_subset = data.Dataset([train_data.examples[i] for i in train_idx], fields=[('text', TEXT), ('label', LABEL)])\n",
    "    # print(len(train_subset))\n",
    "    train_subsets.append(train_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_indices = list(range(len(test_data.examples)))\n",
    "\n",
    "random_seed = 24  \n",
    "kfold_te = KFold(n_splits=k, shuffle=True, random_state=random_seed)\n",
    "\n",
    "test_subsets = []\n",
    "for _, test_idx in kfold_te.split(test_indices):\n",
    "    test_subset = data.Dataset([test_data.examples[i] for i in test_idx], fields=[('text', TEXT), ('label', LABEL)])\n",
    "    # print(len(test_subset))\n",
    "    test_subsets.append(test_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterator(tri_data, tes_data):\n",
    "    \n",
    "    train_iterator, test_iterator = data.BucketIterator.splits(\n",
    "        (tri_data, tes_data),\n",
    "        batch_size = args.B,\n",
    "        sort_key = lambda x: len(x.text),\n",
    "        sort_within_batch=True,\n",
    "        # device=torch.device(\"cpu\") \n",
    "        device=args.device \n",
    "    )\n",
    "\n",
    "    return train_iterator, test_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all_sub = []\n",
    "testa_all_sub = []\n",
    "\n",
    "for index in range(len(train_subsets)) :\n",
    "    data_i, data_j = train_subsets[index ], test_subsets[index ]\n",
    "    data_i_reu, data_j_reu = iterator(data_i, data_j)\n",
    "    train_all_sub.append(data_i_reu)\n",
    "    testa_all_sub.append(data_j_reu)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下面就是之前的单个文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsz = 50\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "clients_wind = ['Task' + str(i) for i in range(0, 20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.BCEWithLogitsLoss().to(args.device) \n",
    "\n",
    "def get_val_loss(model, Vall):  \n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    with torch.no_grad():\n",
    "        for seqq in Vall:\n",
    "            y_preda = model(seqq.text).squeeze(1)\n",
    "            loss = loss_function(y_preda, seqq.label)\n",
    "            val_loss.append(loss.item())\n",
    "    loss_v = np.array(val_loss).mean()\n",
    "    # print('get_val_loss ----- val_loss',val_loss)\n",
    "    # print('get_val_loss ----- np.array(val_loss).mean()', loss_v)\n",
    "    return loss_v \n",
    "\n",
    "\n",
    "def binary_acc(preds, y):\n",
    "    \"\"\"\n",
    "    get accuracy\n",
    "    \"\"\"\n",
    "    preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = torch.eq(preds, y).float()\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "\n",
    "def train(args, model, k):  \n",
    "    model.train()\n",
    "    # train_all_sub\n",
    "    # testa_all_sub\n",
    "    Dtr = train_all_sub[k]   \n",
    "    Val = testa_all_sub[k]\n",
    "    model.Len = len(Dtr) \n",
    "    # print('model.name', model.name)\n",
    "    # print('model.len:', model.Len)\n",
    "\n",
    "    lr = args.lr\n",
    "    if args.optimizer == 'adam': \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3,\n",
    "                                     weight_decay=args.weight_decay)  \n",
    "    else:\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=1e-3,\n",
    "                                    momentum=0.9, weight_decay=args.weight_decay)\n",
    "    \n",
    "    # lr_step = StepLR(optimizer, step_size=args.step_size, gamma=args.gamma) \n",
    "    # training\n",
    "    min_epochs = 1\n",
    "    best_model = None\n",
    "    # min_val_loss = 10\n",
    "    # min_loss_list = [100]\n",
    "    min_val_loss = 10000.0\n",
    "\n",
    "    for epoch in range(args.E):  \n",
    "        train_loss = [] \n",
    "        for seq in Dtr:\n",
    "            # print(seq)\n",
    "            # seq = seq.to(device) \n",
    "            y_pred = model(seq.text).squeeze(1)\n",
    "            loss = loss_function(y_pred, seq.label)\n",
    "            train_loss.append(loss.item())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    # lr_step.step()\n",
    "\n",
    "        val_loss = get_val_loss(model, Val)\n",
    "        if (file_args_key == \"shake\") and ((epoch+1) % 1 == 0) :\n",
    "            # print('epoch {:03d} 训练 train_loss {:.8f} 验证 val_loss {:.8f}'.format(epoch, np.array(train_loss).mean(), val_loss))\n",
    "            wstring = f\"epoch {epoch} 训练 train_loss {np.array(train_loss).mean()} 验证 val_loss {val_loss}\"\n",
    "            writecon = str(wstring) + \"\\n\"\n",
    "            file_write_log.write(writecon)\n",
    "\n",
    "        if (file_args_key == \"live\") and ((epoch+1) % 1 == 0):\n",
    "            # print('epoch {:03d} 训练 train_loss {:.8f} 验证 val_loss {:.8f}'.format(epoch, np.array(train_loss).mean(), val_loss))\n",
    "            wstring = f\"epoch {epoch} 训练 train_loss {np.array(train_loss).mean()} 验证 val_loss {val_loss}\"\n",
    "            writecon = str(wstring) + \"\\n\"\n",
    "            file_write_log.write(writecon)\n",
    "\n",
    "        if (epoch + 1 >= min_epochs) and (val_loss <= min_val_loss) : # 达到最小迭代次数（10），并且损失最优（5）\n",
    "            min_val_loss = val_loss\n",
    "        # validation\n",
    "        model.train()\n",
    "\n",
    "    avg_val_lossa = np.array(train_loss).mean()\n",
    "    wstring = f\"model {k} val_loss: {avg_val_lossa} \"\n",
    "    writecon = str(wstring) + \"\\n\"\n",
    "    file_write_log.write(writecon)\n",
    "    # print('epoch {:03d} train_loss {:.8f} val_loss ...'.format(epoch, np.array(train_loss).mean()))\n",
    "    wstring = f\"训练 best min train_loss {min_val_loss} \"\n",
    "    writecon = str(wstring) + \"\\n\"\n",
    "    file_write_log.write(writecon)\n",
    "    print('model {}, avg_loss {:.8f},  min_loss {:.8f}, val loss {:.8f} '.format(k, avg_val_lossa,min_val_loss, val_loss ))\n",
    "        # if (epoch+1) % 5 == 0:\n",
    "        #     print('epoch {:03d} 训练 train_loss {:.8f} 验证 val_loss {:.8f}'.format(epoch, np.array(train_loss).mean(), val_loss))\n",
    "    # return best_model\n",
    "    return model\n",
    "\n",
    "def test(args, model, k):\n",
    "    model.eval()\n",
    "    Val = testa_all_sub[k] \n",
    "    # pred = []\n",
    "    # y = []\n",
    "    avg_acc = []\n",
    "    # device = device\n",
    "    with torch.no_grad():\n",
    "        for seq in Val:\n",
    "            # seq = seq.to(device)  \n",
    "            y_pred = model(seq.text).squeeze(1) \n",
    "            loss = loss_function(y_pred, seq.label)\n",
    "            acc = binary_acc(y_pred, seq.label).item()\n",
    "            avg_acc.append(acc)\n",
    "    avg_acc = np.mean(avg_acc)\n",
    "    print(\"客户端：\", k, ' avg acc', avg_acc)\n",
    "    wstring = f\"客户端：{k} avg acc: {avg_acc}\"\n",
    "    writecon = str(wstring) + \"\\n\"\n",
    "    file_write_log.write(writecon)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FedAvg: \n",
    "\n",
    "    def __init__(self, args):\n",
    "        self.args = args \n",
    "        self.clients = args.clients  # ['Task_' + str(i) for i in range(0, 20)]\n",
    "\n",
    "        # self.nn = RNN(vocab_size, args, name='server').to(device)   \n",
    "        # self.nn = RFFMRNN(vocab_size, args, name='server').to(device)  \n",
    "        if yesorno_inputdim == 50:\n",
    "            self.nn = RNN50(vocab_size, args, name='server').to(args.device)   \n",
    "            self.nn.embedding.weight.data.copy_(pretrained_embedding)\n",
    "        elif (proof_number == 1) or (proof_number == 3) or (proof_number == 6): \n",
    "            self.nn = RNN50(vocab_size, args, name='server').to(args.device)   \n",
    "            self.nn.embedding.weight.data.copy_(new_pretrained_embedding)\n",
    "        elif (proof_number == 0) or (proof_number == 2) :\n",
    "            self.nn = RNN(vocab_size, args, name='server').to(args.device)  \n",
    "            self.nn.embedding.weight.data.copy_(pretrained_embedding)\n",
    "\n",
    "        self.nns = []\n",
    "\n",
    "        for i in range(self.args.K):    \n",
    "            temp = copy.deepcopy(self.nn)\n",
    "            temp.name = self.clients[i]  \n",
    "            self.nns.append(temp)  \n",
    "    \n",
    "\n",
    "\n",
    "    def server(self):\n",
    "        for t in range(self.args.r):  \n",
    "            # file.write('round-------------!!!!!!!!!!!!!!!!!!!!!!!!!' + str(t + 1) + ':' + '\\n')\n",
    "            print('round：--------', t + 1, '------------------------')\n",
    "            wstring = f\"round：-------- {t + 1} ------------------------\"\n",
    "            writecon = str(wstring) + \"\\n\"\n",
    "            file_write_log.write(writecon)\n",
    "            m = np.max([int(self.args.C * self.args.K), 1]) \n",
    "            index = random.sample(range(0, self.args.K), m)  \n",
    "            print('选取客户端:',index)\n",
    "            wstring = f\"选取客户端: {index} \"\n",
    "            writecon = str(wstring) + \"\\n\"\n",
    "            file_write_log.write(writecon)\n",
    "            # dispatch\n",
    "            self.dispatch(index) \n",
    "            # local updating\n",
    "            self.client_update(index)  \n",
    "            # aggregation\n",
    "            self.aggregation(index)  \n",
    "\n",
    "        torch.save(self.nn.state_dict(), save_name_pth)\n",
    "        print(save_name_pth, 'save')\n",
    "        return self.nn\n",
    "\n",
    "    def dispatch(self, index): \n",
    "        for j in index:\n",
    "            for old_params, new_params in zip(self.nns[j].parameters(), self.nn.parameters()):\n",
    "                old_params.data = new_params.data.clone()\n",
    "\n",
    "    def client_update(self, index):  \n",
    "        for k in index: \n",
    "            wstring = f\"训练本地模型： {k} \"\n",
    "            writecon = str(wstring) + \"\\n\"\n",
    "            file_write_log.write(writecon)\n",
    "            self.nns[k] = train(self.args, self.nns[k], k)  \n",
    "\n",
    "    def aggregation(self, index):  # 聚合方法\n",
    "        s = 0\n",
    "        for j in index:\n",
    "            net = self.nns[j]\n",
    "            s += net.Len\n",
    "        params = {}\n",
    "        for k, v in self.nns[0].named_parameters(): \n",
    "            params[k] = torch.zeros_like(v.data)\n",
    "\n",
    "        for j in index:\n",
    "            for k, v in self.nns[j].named_parameters(): \n",
    "                params[k] += v.data * (self.nns[j].Len / s)  \n",
    "\n",
    "        for k, v in self.nn.named_parameters():\n",
    "            v.data = params[k].data.clone() \n",
    "\n",
    "\n",
    "    def global_test(self, args): \n",
    "        model = self.nn \n",
    "        model.eval()  \n",
    "        c = clients_wind # ['Task' + str(i) for i in range(0, 20)]\n",
    "        for k, client in enumerate(c):\n",
    "            model.name = client  \n",
    "            # result_acc = test(self.args, model, k)\n",
    "            test(self.args, model, k)\n",
    "            # f.write(str(result_acc) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fed = FedAvg(args)\n",
    "\n",
    "fed.server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fed.global_test(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_write_log.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e3f128cad0f25ad952b91777a639b4fa2222dfd0378c44e8bdbd611468a92202"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
