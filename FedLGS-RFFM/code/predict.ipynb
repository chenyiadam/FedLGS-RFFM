{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_args_key = \"live\"\n",
    "proof_number = 0\n",
    "save_name_pth = \"live_3_8_0_30_0.5_write_08_25_2023_14_26_10_dict.pth\"\n",
    "yesorno_inputdim = 50  \n",
    "out_pth = r\"D:\\out_pth\" \n",
    "save_name_pth = out_pth + \"\\\\\" + save_name_pth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext.legacy import data, datasets\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from model_nn import RNN, RNN50\n",
    "# from model_nn_min import RNN, RNN_gradient,RNN50, RNN50_gradient\n",
    "from args_fvg import args_parser_live, args_parser_shake\n",
    "\n",
    "from nltk.tag import pos_tag\n",
    "import re\n",
    "import time\n",
    "\n",
    "\n",
    "if file_args_key == \"shake\":\n",
    "    args = args_parser_shake()\n",
    "elif file_args_key == \"live\":\n",
    "    args = args_parser_live()\n",
    "\n",
    "TEXT = data.Field(tokenize='spacy')\n",
    "LABEL = data.LabelField(dtype=torch.float)\n",
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
    "\n",
    "if file_args_key == \"shake\":\n",
    "    train_data.examples = train_data.examples[:len(train_data.examples)//125]\n",
    "    test_data.examples = test_data.examples[:len(test_data.examples)//125]\n",
    "\n",
    "len_train_data = len(train_data)\n",
    "len_test_data = len(test_data)\n",
    "\n",
    "print('len of train data:', len(train_data))\n",
    "print('len of test data:', len(test_data))\n",
    "print(test_data.examples[1].text) \n",
    "print(test_data.examples[1].label)\n",
    "\n",
    "# word2vec, glove\n",
    "\n",
    "if yesorno_inputdim == 100:\n",
    "    TEXT.build_vocab(train_data, max_size=10000, vectors='glove.6B.100d')\n",
    "if yesorno_inputdim == 50:\n",
    "    TEXT.build_vocab(train_data, max_size=10000, vectors='glove.6B.50d')\n",
    "\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "# TEXT.build_vocab(train_data, max_size=10000, vectors='glove.6B.50d')\n",
    "# LABEL.build_vocab(train_data)\n",
    "\n",
    "# TEXT.build_vocab(test_data, max_size=10000, vectors='glove.6B.100d')\n",
    "# LABEL.build_vocab(test_data)\n",
    "\n",
    "vocab_size =  len(TEXT.vocab)\n",
    "pretrained_embedding = TEXT.vocab.vectors\n",
    "print(vocab_size, pretrained_embedding.shape)\n",
    "\n",
    "def RFFM(i, x):\n",
    "    torch.manual_seed(i)\n",
    "    input_dim = 100\n",
    "    output_dim = 50\n",
    "    sigma = 1.0\n",
    "    omega = torch.randn(output_dim, input_dim) * sigma\n",
    "    b = torch.rand(output_dim) * 2 * np.pi\n",
    "    x = x.view(-1, input_dim)\n",
    "    z = torch.cos(torch.mm(x, omega.t()) + b)\n",
    "    Z_Out =  torch.sqrt(2.0 / torch.tensor(output_dim)) * z\n",
    "    return Z_Out\n",
    "\n",
    "if yesorno_inputdim == 50:\n",
    "    model = RNN50(vocab_size, args, name='server').to(args.device)\n",
    "    model.embedding.weight.data.copy_(pretrained_embedding)\n",
    "\n",
    "elif (proof_number == 1) or (proof_number == 3) or (proof_number == 6):  \n",
    "    new_vectors = []\n",
    "    for i, word in enumerate(TEXT.vocab.itos):\n",
    "        old_vector = pretrained_embedding[i]\n",
    "        new_vector = RFFM(i, old_vector)\n",
    "        new_vectors.append(new_vector)\n",
    "    new_pretrained_embedding = torch.stack(new_vectors, dim=0)\n",
    "    new_pretrained_embedding = new_pretrained_embedding.reshape(-1,50)\n",
    "    print(new_pretrained_embedding.shape)\n",
    "    \n",
    "    model = RNN50(vocab_size, args, name='server').to(args.device)   # \n",
    "    model.embedding.weight.data.copy_(new_pretrained_embedding)\n",
    "            \n",
    "elif (proof_number == 2) or (proof_number == 0) :\n",
    "    model = RNN(vocab_size, args, name='server').to(args.device)\n",
    "    model.embedding.weight.data.copy_(pretrained_embedding)\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "['Actor', 'turned', 'director', 'Bill', 'Paxton', 。。。\n",
    "pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(save_name_pth))\n",
    "model.eval()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_indices = list(range(len(train_data.examples)))\n",
    "k = args.K \n",
    "random_seed = 42  \n",
    "kfold = KFold(n_splits=k, shuffle=True, random_state=random_seed)\n",
    "train_subsets = []\n",
    "for _, train_idx in kfold.split(train_indices):\n",
    "    train_subset = data.Dataset([train_data.examples[i] for i in train_idx], fields=[('text', TEXT), ('label', LABEL)])\n",
    "    # print(len(train_subset))\n",
    "    train_subsets.append(train_subset)\n",
    "\n",
    "test_indices = list(range(len(test_data.examples)))\n",
    "random_seed = 24 \n",
    "kfold_te = KFold(n_splits=k, shuffle=True, random_state=random_seed)\n",
    "test_subsets = []\n",
    "for _, test_idx in kfold_te.split(test_indices):\n",
    "    test_subset = data.Dataset([test_data.examples[i] for i in test_idx], fields=[('text', TEXT), ('label', LABEL)])\n",
    "    # print(len(test_subset))\n",
    "    test_subsets.append(test_subset)\n",
    "\n",
    "\n",
    "def iterator(tri_data, tes_data):\n",
    "    train_iterator, test_iterator = data.BucketIterator.splits(\n",
    "        (tri_data, tes_data),\n",
    "        batch_size = args.B,\n",
    "        sort_key = lambda x: len(x.text),\n",
    "        sort_within_batch=True,\n",
    "        device=args.device\n",
    "    )\n",
    "    return train_iterator, test_iterator\n",
    "\n",
    "train_all_sub = []\n",
    "testa_all_sub = []\n",
    "\n",
    "for index in range(len(train_subsets)) :\n",
    "    data_i, data_j = train_subsets[index ], test_subsets[index ]\n",
    "    data_i_reu, data_j_reu = iterator(data_i, data_j)\n",
    "    train_all_sub.append(data_i_reu)\n",
    "    testa_all_sub.append(data_j_reu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve,auc\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "loss_function = nn.BCEWithLogitsLoss().to(args.device) \n",
    "\n",
    "def binary_acc(preds, y):\n",
    "    preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = torch.eq(preds, y).float()\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "def test(args, model, k):\n",
    "    model.eval()\n",
    "    Val = testa_all_sub[k] \n",
    "    avg_acc = []\n",
    "    avg_auc = []\n",
    "    avg_ks = []\n",
    "    avg_precision = []\n",
    "    avg_recall = []\n",
    "    avg_f1 = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for seq in Val:\n",
    "            y_pred = model(seq.text).squeeze(1)\n",
    "            acc = binary_acc(y_pred, seq.label).item()\n",
    "            avg_acc.append(acc)\n",
    "            auc = roc_auc_score(seq.label.cpu(), torch.sigmoid(y_pred).cpu())\n",
    "            avg_auc.append(auc)\n",
    "            fpr, tpr, thresholds = roc_curve(seq.label.cpu(), torch.sigmoid(y_pred).cpu())\n",
    "            ks = np.max(np.abs(tpr - fpr))\n",
    "            avg_ks.append(ks)\n",
    "            y_pred_binary = torch.round(torch.sigmoid(y_pred)).cpu().numpy()\n",
    "            precision = precision_score(seq.label.cpu(), y_pred_binary)\n",
    "            recall = recall_score(seq.label.cpu(), y_pred_binary)\n",
    "            f1 = f1_score(seq.label.cpu(), y_pred_binary)\n",
    "            avg_precision.append(precision)\n",
    "            avg_recall.append(recall)\n",
    "            avg_f1.append(f1)\n",
    "            \n",
    "    avg_acc = np.mean(avg_acc)\n",
    "    avg_auc = np.mean(avg_auc)\n",
    "    avg_ks = np.mean(avg_ks)\n",
    "    avg_precision = np.mean(avg_precision)\n",
    "    avg_recall = np.mean(avg_recall)\n",
    "    avg_f1 = np.mean(avg_f1)\n",
    "    \n",
    "    # print('客户端 %s avg acc:' % k, avg_acc)\n",
    "    # print('客户端 %s avg auc:' % k, avg_auc)\n",
    "    # print('客户端 %s avg ks:' % k, avg_ks)\n",
    "    # print('客户端 %s avg precision:' % k, avg_precision)\n",
    "    # print('客户端 %s avg recall:' % k, avg_recall)\n",
    "    # print('客户端 %s avg f1:' % k, avg_f1)\n",
    "    # print(\"\\n\")\n",
    "\n",
    "    print(f'客户端 {k} avg acc: {avg_acc}, avg auc: {avg_auc}')\n",
    "\n",
    "# test(args, model, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(len(args.clients)):\n",
    "    with torch.no_grad():\n",
    "        test(args, model, k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e3f128cad0f25ad952b91777a639b4fa2222dfd0378c44e8bdbd611468a92202"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
